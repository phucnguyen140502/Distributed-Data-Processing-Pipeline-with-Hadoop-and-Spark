[2024-10-05T01:48:40.418+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T01:48:40.421+0700] {executor_loader.py:254} INFO - Loaded executor: SequentialExecutor
[2024-10-05T01:48:40.520+0700] {scheduler_job_runner.py:935} INFO - Starting the scheduler
[2024-10-05T01:48:40.524+0700] {scheduler_job_runner.py:942} INFO - Processing each file at most -1 times
[2024-10-05T01:48:40.541+0700] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 187982
[2024-10-05T01:48:40.548+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T01:48:40.549+0700] {settings.py:63} INFO - Configured default timezone UTC
[2024-10-05T01:48:40.589+0700] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-10-05T01:53:40.850+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T01:58:41.246+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T02:03:41.324+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T02:08:41.376+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T02:13:41.413+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T02:18:41.752+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2024-10-04 19:22:42.653551+00:00 hash info: bbd4e9b9cae3d105913dfb0afc2e1c95
[2024-10-05T02:22:44.089+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: example_branch_datetime_operator.datetime_branch manual__2024-10-04T19:22:42.550371+00:00 [scheduled]>
[2024-10-05T02:22:44.090+0700] {scheduler_job_runner.py:495} INFO - DAG example_branch_datetime_operator has 1/16 running and queued tasks
[2024-10-05T02:22:44.090+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: example_branch_datetime_operator.datetime_branch manual__2024-10-04T19:22:42.550371+00:00 [scheduled]>
[2024-10-05T02:22:44.093+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: example_branch_datetime_operator.datetime_branch manual__2024-10-04T19:22:42.550371+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T02:22:44.094+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_branch_datetime_operator', task_id='datetime_branch', run_id='manual__2024-10-04T19:22:42.550371+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2024-10-05T02:22:44.094+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_branch_datetime_operator', 'datetime_branch', 'manual__2024-10-04T19:22:42.550371+00:00', '--local', '--subdir', '/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_branch_datetime_operator.py']
[2024-10-05T02:22:44.098+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_branch_datetime_operator', 'datetime_branch', 'manual__2024-10-04T19:22:42.550371+00:00', '--local', '--subdir', '/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_branch_datetime_operator.py']
[2024-10-05T02:22:46.041+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T02:22:46.070+0700] {dagbag.py:588} INFO - Filling up the DagBag from /usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_branch_datetime_operator.py
[2024-10-05T02:22:46.239+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T02:22:46.809+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T02:22:46.810+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T02:22:46.818+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T02:22:46.851+0700] {task_command.py:467} INFO - Running <TaskInstance: example_branch_datetime_operator.datetime_branch manual__2024-10-04T19:22:42.550371+00:00 [queued]> on host linux-ip-147
[2024-10-05T02:22:47.819+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='example_branch_datetime_operator', task_id='datetime_branch', run_id='manual__2024-10-04T19:22:42.550371+00:00', try_number=1, map_index=-1)
[2024-10-05T02:22:47.832+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_branch_datetime_operator, task_id=datetime_branch, run_id=manual__2024-10-04T19:22:42.550371+00:00, map_index=-1, run_start_date=2024-10-04 19:22:46.900628+00:00, run_end_date=2024-10-04 19:22:47.136675+00:00, run_duration=0.236047, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=4, pool=default_pool, queue=default, priority_weight=3, operator=BranchDateTimeOperator, queued_dttm=2024-10-04 19:22:44.091783+00:00, queued_by_job_id=1, pid=190019
[2024-10-05T02:22:48.427+0700] {dagrun.py:854} INFO - Marking run <DagRun example_branch_datetime_operator @ 2024-10-04 19:22:42.550371+00:00: manual__2024-10-04T19:22:42.550371+00:00, state:running, queued_at: 2024-10-04 19:22:42.653551+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-04 19:22:44.034209+00:00 end:2024-10-04 19:22:48.429067+00:00
[2024-10-05T02:22:48.429+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=example_branch_datetime_operator, execution_date=2024-10-04 19:22:42.550371+00:00, run_id=manual__2024-10-04T19:22:42.550371+00:00, run_start_date=2024-10-04 19:22:44.034209+00:00, run_end_date=2024-10-04 19:22:48.429067+00:00, run_duration=4.394858, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-03 00:00:00+00:00, data_interval_end=2024-10-04 00:00:00+00:00, dag_hash=bbd4e9b9cae3d105913dfb0afc2e1c95
[2024-10-05T02:23:42.163+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T02:28:42.570+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T02:28:42.577+0700] {scheduler_job_runner.py:1870} INFO - Marked 1 SchedulerJob instances as failed
[2024-10-05T02:33:42.742+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T02:38:42.982+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T02:43:43.186+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T02:48:22.965+0700] {manager.py:537} INFO - DAG talend_job_dag is missing and will be deactivated.
[2024-10-05T02:48:22.977+0700] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2024-10-05T02:48:22.985+0700] {manager.py:553} INFO - Deleted DAG talend_job_dag in serialized_dag table
[2024-10-05T02:48:43.339+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2024-10-04 19:49:14.470152+00:00 hash info: c160c7a79b06741fb2bef50b6789b7a9
[2024-10-05T02:49:15.693+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-04 19:49:14.415475+00:00: manual__2024-10-04T19:49:14.415475+00:00, state:running, queued_at: 2024-10-04 19:49:14.470152+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-04 19:49:15.681072+00:00 end:2024-10-04 19:49:15.694068+00:00
[2024-10-05T02:49:15.694+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-04 19:49:14.415475+00:00, run_id=manual__2024-10-04T19:49:14.415475+00:00, run_start_date=2024-10-04 19:49:15.681072+00:00, run_end_date=2024-10-04 19:49:15.694068+00:00, run_duration=0.012996, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-03 02:45:00+00:00, data_interval_end=2024-10-04 02:45:00+00:00, dag_hash=c160c7a79b06741fb2bef50b6789b7a9
Dag run  in running state
Dag information Queued at: 2024-10-04 19:49:52.724719+00:00 hash info: c160c7a79b06741fb2bef50b6789b7a9
[2024-10-05T02:49:53.749+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-04 19:49:52.684361+00:00: manual__2024-10-04T19:49:52.684361+00:00, state:running, queued_at: 2024-10-04 19:49:52.724719+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-04 19:49:53.730401+00:00 end:2024-10-04 19:49:53.751041+00:00
[2024-10-05T02:49:53.751+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-04 19:49:52.684361+00:00, run_id=manual__2024-10-04T19:49:52.684361+00:00, run_start_date=2024-10-04 19:49:53.730401+00:00, run_end_date=2024-10-04 19:49:53.751041+00:00, run_duration=0.02064, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-03 02:45:00+00:00, data_interval_end=2024-10-04 02:45:00+00:00, dag_hash=c160c7a79b06741fb2bef50b6789b7a9
[2024-10-05T02:53:43.528+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T02:58:43.912+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2024-10-04 20:01:04.363332+00:00 hash info: 345e84b676dc1cee7edda0426fa22eb2
[2024-10-05T03:01:06.035+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-04 20:01:04.310004+00:00: manual__2024-10-04T20:01:04.310004+00:00, state:running, queued_at: 2024-10-04 20:01:04.363332+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-04 20:01:06.011415+00:00 end:2024-10-04 20:01:06.037640+00:00
[2024-10-05T03:01:06.037+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-04 20:01:04.310004+00:00, run_id=manual__2024-10-04T20:01:04.310004+00:00, run_start_date=2024-10-04 20:01:06.011415+00:00, run_end_date=2024-10-04 20:01:06.037640+00:00, run_duration=0.026225, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-03 03:00:00+00:00, data_interval_end=2024-10-04 03:00:00+00:00, dag_hash=345e84b676dc1cee7edda0426fa22eb2
[2024-10-05T03:03:44.023+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T03:08:44.213+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2024-10-04 20:12:37.762404+00:00 hash info: 40228cffe6290e99661d5ba6531fd3a8
[2024-10-05T03:12:38.506+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-04 20:12:37.722482+00:00: manual__2024-10-04T20:12:37.722482+00:00, state:running, queued_at: 2024-10-04 20:12:37.762404+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-04 20:12:38.485327+00:00 end:2024-10-04 20:12:38.508071+00:00
[2024-10-05T03:12:38.508+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-04 20:12:37.722482+00:00, run_id=manual__2024-10-04T20:12:37.722482+00:00, run_start_date=2024-10-04 20:12:38.485327+00:00, run_end_date=2024-10-04 20:12:38.508071+00:00, run_duration=0.022744, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-03 03:15:00+00:00, data_interval_end=2024-10-04 03:15:00+00:00, dag_hash=40228cffe6290e99661d5ba6531fd3a8
[2024-10-05T03:13:44.597+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T03:18:45.089+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T03:23:45.466+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2024-10-04 20:28:39.017207+00:00 hash info: 40228cffe6290e99661d5ba6531fd3a8
[2024-10-05T03:28:39.084+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-04 20:28:38.978623+00:00: manual__2024-10-04T20:28:38.978623+00:00, state:running, queued_at: 2024-10-04 20:28:39.017207+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-04 20:28:39.062520+00:00 end:2024-10-04 20:28:39.086751+00:00
[2024-10-05T03:28:39.087+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-04 20:28:38.978623+00:00, run_id=manual__2024-10-04T20:28:38.978623+00:00, run_start_date=2024-10-04 20:28:39.062520+00:00, run_end_date=2024-10-04 20:28:39.086751+00:00, run_duration=0.024231, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-03 03:15:00+00:00, data_interval_end=2024-10-04 03:15:00+00:00, dag_hash=40228cffe6290e99661d5ba6531fd3a8
Dag run  in running state
Dag information Queued at: 2024-10-04 20:28:43.686012+00:00 hash info: 40228cffe6290e99661d5ba6531fd3a8
[2024-10-05T03:28:44.271+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-04 20:28:43.598391+00:00: manual__2024-10-04T20:28:43.598391+00:00, state:running, queued_at: 2024-10-04 20:28:43.686012+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-04 20:28:44.257207+00:00 end:2024-10-04 20:28:44.272486+00:00
[2024-10-05T03:28:44.272+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-04 20:28:43.598391+00:00, run_id=manual__2024-10-04T20:28:43.598391+00:00, run_start_date=2024-10-04 20:28:44.257207+00:00, run_end_date=2024-10-04 20:28:44.272486+00:00, run_duration=0.015279, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-03 03:15:00+00:00, data_interval_end=2024-10-04 03:15:00+00:00, dag_hash=40228cffe6290e99661d5ba6531fd3a8
[2024-10-05T03:28:45.885+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2024-10-04 20:28:45.146262+00:00 hash info: 40228cffe6290e99661d5ba6531fd3a8
[2024-10-05T03:28:47.185+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-04 20:28:45.121852+00:00: manual__2024-10-04T20:28:45.121852+00:00, state:running, queued_at: 2024-10-04 20:28:45.146262+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-04 20:28:47.162460+00:00 end:2024-10-04 20:28:47.186305+00:00
[2024-10-05T03:28:47.186+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-04 20:28:45.121852+00:00, run_id=manual__2024-10-04T20:28:45.121852+00:00, run_start_date=2024-10-04 20:28:47.162460+00:00, run_end_date=2024-10-04 20:28:47.186305+00:00, run_duration=0.023845, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-03 03:15:00+00:00, data_interval_end=2024-10-04 03:15:00+00:00, dag_hash=40228cffe6290e99661d5ba6531fd3a8
Dag run  in running state
Dag information Queued at: 2024-10-04 20:31:45.467677+00:00 hash info: 40228cffe6290e99661d5ba6531fd3a8
[2024-10-05T03:31:47.396+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-04 20:31:45.433621+00:00: manual__2024-10-04T20:31:45.433621+00:00, state:running, queued_at: 2024-10-04 20:31:45.467677+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-04 20:31:47.380405+00:00 end:2024-10-04 20:31:47.398228+00:00
[2024-10-05T03:31:47.398+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-04 20:31:45.433621+00:00, run_id=manual__2024-10-04T20:31:45.433621+00:00, run_start_date=2024-10-04 20:31:47.380405+00:00, run_end_date=2024-10-04 20:31:47.398228+00:00, run_duration=0.017823, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-03 03:15:00+00:00, data_interval_end=2024-10-04 03:15:00+00:00, dag_hash=40228cffe6290e99661d5ba6531fd3a8
Dag run  in running state
Dag information Queued at: 2024-10-04 20:32:37.840885+00:00 hash info: 40228cffe6290e99661d5ba6531fd3a8
[2024-10-05T03:32:39.161+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-04 20:32:37.805214+00:00: manual__2024-10-04T20:32:37.805214+00:00, state:running, queued_at: 2024-10-04 20:32:37.840885+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-04 20:32:39.135894+00:00 end:2024-10-04 20:32:39.163065+00:00
[2024-10-05T03:32:39.163+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-04 20:32:37.805214+00:00, run_id=manual__2024-10-04T20:32:37.805214+00:00, run_start_date=2024-10-04 20:32:39.135894+00:00, run_end_date=2024-10-04 20:32:39.163065+00:00, run_duration=0.027171, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-03 03:15:00+00:00, data_interval_end=2024-10-04 03:15:00+00:00, dag_hash=40228cffe6290e99661d5ba6531fd3a8
Dag run  in running state
Dag information Queued at: 2024-10-04 20:33:06.413363+00:00 hash info: 40228cffe6290e99661d5ba6531fd3a8
[2024-10-05T03:33:07.800+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-04 20:33:06.386431+00:00: manual__2024-10-04T20:33:06.386431+00:00, state:running, queued_at: 2024-10-04 20:33:06.413363+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-04 20:33:07.778389+00:00 end:2024-10-04 20:33:07.802199+00:00
[2024-10-05T03:33:07.802+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-04 20:33:06.386431+00:00, run_id=manual__2024-10-04T20:33:06.386431+00:00, run_start_date=2024-10-04 20:33:07.778389+00:00, run_end_date=2024-10-04 20:33:07.802199+00:00, run_duration=0.02381, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-03 03:15:00+00:00, data_interval_end=2024-10-04 03:15:00+00:00, dag_hash=40228cffe6290e99661d5ba6531fd3a8
Dag run  in running state
Dag information Queued at: 2024-10-04 20:33:08.059012+00:00 hash info: 40228cffe6290e99661d5ba6531fd3a8
[2024-10-05T03:33:09.218+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-04 20:33:08.027963+00:00: manual__2024-10-04T20:33:08.027963+00:00, state:running, queued_at: 2024-10-04 20:33:08.059012+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-04 20:33:09.196423+00:00 end:2024-10-04 20:33:09.219832+00:00
[2024-10-05T03:33:09.220+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-04 20:33:08.027963+00:00, run_id=manual__2024-10-04T20:33:08.027963+00:00, run_start_date=2024-10-04 20:33:09.196423+00:00, run_end_date=2024-10-04 20:33:09.219832+00:00, run_duration=0.023409, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-03 03:15:00+00:00, data_interval_end=2024-10-04 03:15:00+00:00, dag_hash=40228cffe6290e99661d5ba6531fd3a8
Dag run  in running state
Dag information Queued at: 2024-10-04 20:33:09.238221+00:00 hash info: 40228cffe6290e99661d5ba6531fd3a8
[2024-10-05T03:33:10.525+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-04 20:33:09.215439+00:00: manual__2024-10-04T20:33:09.215439+00:00, state:running, queued_at: 2024-10-04 20:33:09.238221+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-04 20:33:10.499595+00:00 end:2024-10-04 20:33:10.526919+00:00
[2024-10-05T03:33:10.527+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-04 20:33:09.215439+00:00, run_id=manual__2024-10-04T20:33:09.215439+00:00, run_start_date=2024-10-04 20:33:10.499595+00:00, run_end_date=2024-10-04 20:33:10.526919+00:00, run_duration=0.027324, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-03 03:15:00+00:00, data_interval_end=2024-10-04 03:15:00+00:00, dag_hash=40228cffe6290e99661d5ba6531fd3a8
Dag run  in running state
Dag information Queued at: 2024-10-04 20:33:10.670421+00:00 hash info: 40228cffe6290e99661d5ba6531fd3a8
Dag run  in running state
Dag information Queued at: 2024-10-04 20:33:11.734919+00:00 hash info: 40228cffe6290e99661d5ba6531fd3a8
Dag run  in running state
Dag information Queued at: 2024-10-04 20:33:11.950925+00:00 hash info: 40228cffe6290e99661d5ba6531fd3a8
[2024-10-05T03:33:12.096+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-04 20:33:10.640904+00:00: manual__2024-10-04T20:33:10.640904+00:00, state:running, queued_at: 2024-10-04 20:33:10.670421+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-04 20:33:12.060980+00:00 end:2024-10-04 20:33:12.097697+00:00
[2024-10-05T03:33:12.097+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-04 20:33:10.640904+00:00, run_id=manual__2024-10-04T20:33:10.640904+00:00, run_start_date=2024-10-04 20:33:12.060980+00:00, run_end_date=2024-10-04 20:33:12.097697+00:00, run_duration=0.036717, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-03 03:15:00+00:00, data_interval_end=2024-10-04 03:15:00+00:00, dag_hash=40228cffe6290e99661d5ba6531fd3a8
[2024-10-05T03:33:12.107+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-04 20:33:11.703361+00:00: manual__2024-10-04T20:33:11.703361+00:00, state:running, queued_at: 2024-10-04 20:33:11.734919+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-04 20:33:12.063279+00:00 end:2024-10-04 20:33:12.108279+00:00
[2024-10-05T03:33:12.108+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-04 20:33:11.703361+00:00, run_id=manual__2024-10-04T20:33:11.703361+00:00, run_start_date=2024-10-04 20:33:12.063279+00:00, run_end_date=2024-10-04 20:33:12.108279+00:00, run_duration=0.045, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-03 03:15:00+00:00, data_interval_end=2024-10-04 03:15:00+00:00, dag_hash=40228cffe6290e99661d5ba6531fd3a8
[2024-10-05T03:33:12.114+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-04 20:33:11.908323+00:00: manual__2024-10-04T20:33:11.908323+00:00, state:running, queued_at: 2024-10-04 20:33:11.950925+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-04 20:33:12.063444+00:00 end:2024-10-04 20:33:12.114550+00:00
[2024-10-05T03:33:12.114+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-04 20:33:11.908323+00:00, run_id=manual__2024-10-04T20:33:11.908323+00:00, run_start_date=2024-10-04 20:33:12.063444+00:00, run_end_date=2024-10-04 20:33:12.114550+00:00, run_duration=0.051106, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-03 03:15:00+00:00, data_interval_end=2024-10-04 03:15:00+00:00, dag_hash=40228cffe6290e99661d5ba6531fd3a8
Dag run  in running state
Dag information Queued at: 2024-10-04 20:33:12.064147+00:00 hash info: 40228cffe6290e99661d5ba6531fd3a8
Dag run  in running state
Dag information Queued at: 2024-10-04 20:33:12.211650+00:00 hash info: 40228cffe6290e99661d5ba6531fd3a8
[2024-10-05T03:33:13.477+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-04 20:33:12.039598+00:00: manual__2024-10-04T20:33:12.039598+00:00, state:running, queued_at: 2024-10-04 20:33:12.064147+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-04 20:33:13.453826+00:00 end:2024-10-04 20:33:13.479081+00:00
[2024-10-05T03:33:13.479+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-04 20:33:12.039598+00:00, run_id=manual__2024-10-04T20:33:12.039598+00:00, run_start_date=2024-10-04 20:33:13.453826+00:00, run_end_date=2024-10-04 20:33:13.479081+00:00, run_duration=0.025255, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-03 03:15:00+00:00, data_interval_end=2024-10-04 03:15:00+00:00, dag_hash=40228cffe6290e99661d5ba6531fd3a8
[2024-10-05T03:33:13.490+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-04 20:33:12.193119+00:00: manual__2024-10-04T20:33:12.193119+00:00, state:running, queued_at: 2024-10-04 20:33:12.211650+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-04 20:33:13.454355+00:00 end:2024-10-04 20:33:13.491631+00:00
[2024-10-05T03:33:13.491+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-04 20:33:12.193119+00:00, run_id=manual__2024-10-04T20:33:12.193119+00:00, run_start_date=2024-10-04 20:33:13.454355+00:00, run_end_date=2024-10-04 20:33:13.491631+00:00, run_duration=0.037276, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-03 03:15:00+00:00, data_interval_end=2024-10-04 03:15:00+00:00, dag_hash=40228cffe6290e99661d5ba6531fd3a8
[2024-10-05T03:33:46.271+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T03:38:46.463+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2024-10-04 20:38:56.594263+00:00 hash info: 40228cffe6290e99661d5ba6531fd3a8
[2024-10-05T03:38:58.121+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-04 20:38:56.565538+00:00: manual__2024-10-04T20:38:56.565538+00:00, state:running, queued_at: 2024-10-04 20:38:56.594263+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-04 20:38:58.107439+00:00 end:2024-10-04 20:38:58.122312+00:00
[2024-10-05T03:38:58.122+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-04 20:38:56.565538+00:00, run_id=manual__2024-10-04T20:38:56.565538+00:00, run_start_date=2024-10-04 20:38:58.107439+00:00, run_end_date=2024-10-04 20:38:58.122312+00:00, run_duration=0.014873, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-03 03:15:00+00:00, data_interval_end=2024-10-04 03:15:00+00:00, dag_hash=40228cffe6290e99661d5ba6531fd3a8
Dag run  in running state
Dag information Queued at: 2024-10-04 20:40:14.559177+00:00 hash info: 40228cffe6290e99661d5ba6531fd3a8
[2024-10-05T03:40:15.658+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-04 20:40:14.528966+00:00: manual__2024-10-04T20:40:14.528966+00:00, state:running, queued_at: 2024-10-04 20:40:14.559177+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-04 20:40:15.633898+00:00 end:2024-10-04 20:40:15.660775+00:00
[2024-10-05T03:40:15.661+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-04 20:40:14.528966+00:00, run_id=manual__2024-10-04T20:40:14.528966+00:00, run_start_date=2024-10-04 20:40:15.633898+00:00, run_end_date=2024-10-04 20:40:15.660775+00:00, run_duration=0.026877, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-03 03:15:00+00:00, data_interval_end=2024-10-04 03:15:00+00:00, dag_hash=40228cffe6290e99661d5ba6531fd3a8
[2024-10-05T03:43:46.859+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T03:48:47.380+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T03:53:47.718+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2024-10-04 20:58:18.013592+00:00 hash info: 8ef6c18d5f2d34c5dd7add46397dd4cf
[2024-10-05T03:58:18.623+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-04 20:58:17.980936+00:00: manual__2024-10-04T20:58:17.980936+00:00, state:running, queued_at: 2024-10-04 20:58:18.013592+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-04 20:58:18.601328+00:00 end:2024-10-04 20:58:18.625839+00:00
[2024-10-05T03:58:18.626+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-04 20:58:17.980936+00:00, run_id=manual__2024-10-04T20:58:17.980936+00:00, run_start_date=2024-10-04 20:58:18.601328+00:00, run_end_date=2024-10-04 20:58:18.625839+00:00, run_duration=0.024511, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-03 03:45:00+00:00, data_interval_end=2024-10-04 03:45:00+00:00, dag_hash=8ef6c18d5f2d34c5dd7add46397dd4cf
[2024-10-05T03:58:47.747+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2024-10-04 21:01:34.171342+00:00 hash info: 8ef6c18d5f2d34c5dd7add46397dd4cf
[2024-10-05T04:01:35.435+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-04 21:01:34.131430+00:00: manual__2024-10-04T21:01:34.131430+00:00, state:running, queued_at: 2024-10-04 21:01:34.171342+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-04 21:01:35.409881+00:00 end:2024-10-04 21:01:35.438100+00:00
[2024-10-05T04:01:35.438+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-04 21:01:34.131430+00:00, run_id=manual__2024-10-04T21:01:34.131430+00:00, run_start_date=2024-10-04 21:01:35.409881+00:00, run_end_date=2024-10-04 21:01:35.438100+00:00, run_duration=0.028219, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-03 03:45:00+00:00, data_interval_end=2024-10-04 03:45:00+00:00, dag_hash=8ef6c18d5f2d34c5dd7add46397dd4cf
Dag run  in running state
Dag information Queued at: 2024-10-04 21:02:12.746248+00:00 hash info: 60d28af0f6cab8a0fa4fb9b9dcd67f4b
[2024-10-05T04:02:13.305+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-04 21:02:12.709230+00:00: manual__2024-10-04T21:02:12.709230+00:00, state:running, queued_at: 2024-10-04 21:02:12.746248+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-04 21:02:13.283375+00:00 end:2024-10-04 21:02:13.307092+00:00
[2024-10-05T04:02:13.307+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-04 21:02:12.709230+00:00, run_id=manual__2024-10-04T21:02:12.709230+00:00, run_start_date=2024-10-04 21:02:13.283375+00:00, run_end_date=2024-10-04 21:02:13.307092+00:00, run_duration=0.023717, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-03 03:45:00+00:00, data_interval_end=2024-10-04 03:45:00+00:00, dag_hash=60d28af0f6cab8a0fa4fb9b9dcd67f4b
[2024-10-05T04:03:48.029+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2024-10-04 21:06:11.906686+00:00 hash info: 60d28af0f6cab8a0fa4fb9b9dcd67f4b
[2024-10-05T04:06:12.924+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-04 21:06:11+00:00: manual__2024-10-04T21:06:11+00:00, state:running, queued_at: 2024-10-04 21:06:11.906686+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-04 21:06:12.900951+00:00 end:2024-10-04 21:06:12.926278+00:00
[2024-10-05T04:06:12.926+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-04 21:06:11+00:00, run_id=manual__2024-10-04T21:06:11+00:00, run_start_date=2024-10-04 21:06:12.900951+00:00, run_end_date=2024-10-04 21:06:12.926278+00:00, run_duration=0.025327, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-03 03:45:00+00:00, data_interval_end=2024-10-04 03:45:00+00:00, dag_hash=60d28af0f6cab8a0fa4fb9b9dcd67f4b
Dag run  in running state
Dag information Queued at: 2024-10-04 21:06:33.114420+00:00 hash info: 60d28af0f6cab8a0fa4fb9b9dcd67f4b
[2024-10-05T04:06:34.428+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-04 21:06:33+00:00: manual__2024-10-04T21:06:33+00:00, state:running, queued_at: 2024-10-04 21:06:33.114420+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-04 21:06:34.403108+00:00 end:2024-10-04 21:06:34.430986+00:00
[2024-10-05T04:06:34.431+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-04 21:06:33+00:00, run_id=manual__2024-10-04T21:06:33+00:00, run_start_date=2024-10-04 21:06:34.403108+00:00, run_end_date=2024-10-04 21:06:34.430986+00:00, run_duration=0.027878, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-03 03:45:00+00:00, data_interval_end=2024-10-04 03:45:00+00:00, dag_hash=60d28af0f6cab8a0fa4fb9b9dcd67f4b
Dag run  in running state
Dag information Queued at: 2024-10-04 21:08:09.626360+00:00 hash info: 428a3c034f481cb90f1312a52a548bd4
[2024-10-05T04:08:10.426+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-04 21:08:09+00:00: manual__2024-10-04T21:08:09+00:00, state:running, queued_at: 2024-10-04 21:08:09.626360+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-04 21:08:10.405093+00:00 end:2024-10-04 21:08:10.428728+00:00
[2024-10-05T04:08:10.428+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-04 21:08:09+00:00, run_id=manual__2024-10-04T21:08:09+00:00, run_start_date=2024-10-04 21:08:10.405093+00:00, run_end_date=2024-10-04 21:08:10.428728+00:00, run_duration=0.023635, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-03 03:45:00+00:00, data_interval_end=2024-10-04 03:45:00+00:00, dag_hash=428a3c034f481cb90f1312a52a548bd4
[2024-10-05T04:08:48.336+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T04:13:48.563+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T04:18:48.869+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T04:23:49.251+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T04:28:49.634+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T04:33:50.038+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T04:38:50.535+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T04:43:50.935+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T04:48:51.139+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T04:53:51.343+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T04:58:51.554+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T05:03:51.670+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T05:08:51.891+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T05:13:52.271+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T05:18:52.403+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T05:23:52.799+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T05:28:53.198+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T05:33:53.390+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T05:38:53.423+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T05:43:53.633+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T05:48:53.916+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T05:53:54.434+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T05:58:54.881+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T06:03:54.976+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T06:08:55.383+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T06:13:55.572+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T06:18:55.664+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T06:23:56.052+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T06:28:56.343+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T06:33:56.737+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T06:38:56.952+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T06:43:57.151+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T06:48:57.548+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T06:53:57.784+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T06:58:57.985+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T07:03:58.001+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T07:08:58.290+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T07:13:58.561+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T07:18:58.754+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T07:23:59.159+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T07:28:59.194+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T07:33:59.482+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T07:38:59.674+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T07:44:00.092+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T07:49:00.484+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T07:54:00.960+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T07:59:01.243+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T08:04:01.453+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T08:09:01.653+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T08:14:01.852+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T08:19:02.254+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T08:24:02.404+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T08:29:02.532+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T08:34:02.942+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T08:39:03.215+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T08:44:03.620+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T08:49:03.899+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T08:54:04.297+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T08:59:04.680+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T09:04:05.060+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T09:09:05.335+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T09:14:05.743+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T09:19:06.131+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T09:24:06.533+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T09:29:06.918+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T09:34:07.291+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T09:39:07.495+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T09:44:07.564+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T09:49:07.953+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T09:54:08.334+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T09:59:08.534+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T10:04:08.724+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T10:09:09.108+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T10:14:09.347+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T10:19:09.741+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T10:24:10.146+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T10:29:10.306+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T10:34:10.713+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T10:39:11.117+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T10:44:11.518+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T10:49:11.723+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T10:54:12.107+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T10:59:12.388+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T11:04:12.893+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T11:09:13.190+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T11:14:13.269+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T11:19:13.680+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T11:24:14.089+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T11:29:14.488+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T11:34:14.874+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T11:39:15.252+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T11:44:15.740+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T11:49:16.126+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T11:54:16.400+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T11:59:16.601+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T12:04:16.799+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T12:09:17.204+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T12:14:17.457+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T12:19:17.847+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T12:24:18.246+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T12:29:18.434+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T12:34:18.631+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T12:39:18.860+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T12:44:19.009+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T12:49:19.392+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T12:54:19.564+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T12:59:19.964+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T13:04:20.385+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T13:09:20.572+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T13:14:20.707+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T13:19:20.995+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T13:24:21.375+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T13:29:21.796+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T13:34:21.878+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T13:39:22.331+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T13:44:22.343+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T13:49:22.625+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T13:54:22.909+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T13:59:23.205+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T14:04:23.814+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T14:09:24.227+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T14:14:24.410+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T14:19:24.612+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T14:24:24.894+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T14:29:25.287+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T14:34:25.486+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T14:39:25.730+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T14:44:25.959+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T14:49:26.355+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T14:54:26.763+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T14:59:27.164+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T15:04:27.381+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T15:09:27.727+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T15:14:27.832+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T15:19:28.242+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T15:24:28.671+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T15:29:29.075+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T15:34:29.482+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T15:39:29.682+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T15:44:30.089+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T15:49:30.488+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T15:54:30.892+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T15:59:31.298+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T16:04:31.690+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T16:09:32.086+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T16:14:32.474+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T16:19:32.873+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T16:24:33.275+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T16:29:33.673+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T16:34:33.802+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T16:39:33.955+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T16:44:34.335+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T16:49:34.366+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T16:54:34.780+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T16:59:34.985+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T17:04:35.388+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T17:09:35.777+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T17:14:36.458+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T17:19:36.955+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T17:24:37.164+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T17:29:37.567+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T17:34:37.871+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T17:39:38.080+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T17:44:38.200+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T17:49:38.593+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T17:54:38.800+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T17:59:39.158+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T18:04:39.563+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T18:09:39.843+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T18:14:40.245+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T18:19:40.409+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T18:24:40.707+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2024-10-05 11:25:13.688856+00:00 hash info: 428a3c034f481cb90f1312a52a548bd4
[2024-10-05T18:25:15.086+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:25:13.669229+00:00 [scheduled]>
[2024-10-05T18:25:15.089+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T18:25:15.090+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:25:13.669229+00:00 [scheduled]>
[2024-10-05T18:25:15.094+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:25:13.669229+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T18:25:15.096+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T11:25:13.669229+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T18:25:15.097+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T11:25:13.669229+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T18:25:15.103+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T11:25:13.669229+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T18:25:18.863+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T18:25:18.879+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T18:25:18.953+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: sh /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T18:25:18.994+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T18:25:19.837+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T18:25:19.838+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T18:25:19.876+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T18:25:19.957+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:25:13.669229+00:00 [queued]> on host linux-ip-147
[2024-10-05T18:25:20.752+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T11:25:13.669229+00:00', try_number=1, map_index=-1)
[2024-10-05T18:25:20.765+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T11:25:13.669229+00:00, map_index=-1, run_start_date=2024-10-05 11:25:20.040810+00:00, run_end_date=2024-10-05 11:25:20.226896+00:00, run_duration=0.186086, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=5, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 11:25:15.091999+00:00, queued_by_job_id=1, pid=245794
[2024-10-05T18:29:40.929+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T18:30:20.376+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:25:13.669229+00:00 [scheduled]>
[2024-10-05T18:30:20.378+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T18:30:20.379+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:25:13.669229+00:00 [scheduled]>
[2024-10-05T18:30:20.383+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:25:13.669229+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T18:30:20.385+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T11:25:13.669229+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T18:30:20.386+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T11:25:13.669229+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T18:30:20.391+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T11:25:13.669229+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T18:30:22.439+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T18:30:22.450+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T18:30:22.495+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: sh /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T18:30:22.515+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T18:30:22.905+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T18:30:22.906+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T18:30:22.919+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T18:30:22.949+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:25:13.669229+00:00 [queued]> on host linux-ip-147
[2024-10-05T18:30:24.831+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T11:25:13.669229+00:00', try_number=2, map_index=-1)
[2024-10-05T18:30:24.850+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T11:25:13.669229+00:00, map_index=-1, run_start_date=2024-10-05 11:30:22.996376+00:00, run_end_date=2024-10-05 11:30:23.122597+00:00, run_duration=0.126221, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=6, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 11:30:20.381347+00:00, queued_by_job_id=1, pid=246072
[2024-10-05T18:30:25.280+0700] {dagrun.py:823} ERROR - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-05 11:25:13.669229+00:00: manual__2024-10-05T11:25:13.669229+00:00, state:running, queued_at: 2024-10-05 11:25:13.688856+00:00. externally triggered: True> failed
Dag run  in failure state
Dag information:talend_job_dag_Sync_Mysql_to_DWh_job Run id: manual__2024-10-05T11:25:13.669229+00:00 external trigger: True
Failed with message: task_failure
[2024-10-05T18:30:25.282+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-05 11:25:13.669229+00:00, run_id=manual__2024-10-05T11:25:13.669229+00:00, run_start_date=2024-10-05 11:25:15.014925+00:00, run_end_date=2024-10-05 11:30:25.282002+00:00, run_duration=310.267077, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-10-04 03:45:00+00:00, data_interval_end=2024-10-05 03:45:00+00:00, dag_hash=c7da5c580ea09a58b2ba6ef98a64bdc7
Dag run  in running state
Dag information Queued at: 2024-10-05 11:34:13.055530+00:00 hash info: c7da5c580ea09a58b2ba6ef98a64bdc7
[2024-10-05T18:34:14.856+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:34:13.003458+00:00 [scheduled]>
[2024-10-05T18:34:14.858+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T18:34:14.859+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:34:13.003458+00:00 [scheduled]>
[2024-10-05T18:34:14.863+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:34:13.003458+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T18:34:14.864+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T11:34:13.003458+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T18:34:14.865+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T11:34:13.003458+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T18:34:14.871+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T11:34:13.003458+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T18:34:16.593+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T18:34:16.605+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T18:34:16.651+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: sh /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T18:34:16.671+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T18:34:17.073+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T18:34:17.074+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T18:34:17.086+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T18:34:17.117+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:34:13.003458+00:00 [queued]> on host linux-ip-147
[2024-10-05T18:34:17.927+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T11:34:13.003458+00:00', try_number=1, map_index=-1)
[2024-10-05T18:34:17.934+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T11:34:13.003458+00:00, map_index=-1, run_start_date=2024-10-05 11:34:17.163963+00:00, run_end_date=2024-10-05 11:34:17.302145+00:00, run_duration=0.138182, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=7, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 11:34:14.861055+00:00, queued_by_job_id=1, pid=246300
[2024-10-05T18:34:41.375+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T18:39:18.726+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:34:13.003458+00:00 [scheduled]>
[2024-10-05T18:39:18.727+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T18:39:18.728+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:34:13.003458+00:00 [scheduled]>
[2024-10-05T18:39:18.730+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:34:13.003458+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T18:39:18.731+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T11:34:13.003458+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T18:39:18.732+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T11:34:13.003458+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T18:39:18.737+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T11:34:13.003458+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T18:39:20.475+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T18:39:20.488+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T18:39:20.535+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: sh /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T18:39:20.555+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T18:39:20.962+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T18:39:20.963+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T18:39:20.975+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T18:39:21.009+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:34:13.003458+00:00 [queued]> on host linux-ip-147
[2024-10-05T18:39:22.897+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T11:34:13.003458+00:00', try_number=2, map_index=-1)
[2024-10-05T18:39:22.920+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T11:34:13.003458+00:00, map_index=-1, run_start_date=2024-10-05 11:39:21.056877+00:00, run_end_date=2024-10-05 11:39:21.186095+00:00, run_duration=0.129218, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=8, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 11:39:18.728942+00:00, queued_by_job_id=1, pid=246599
[2024-10-05T18:39:23.381+0700] {dagrun.py:823} ERROR - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-05 11:34:13.003458+00:00: manual__2024-10-05T11:34:13.003458+00:00, state:running, queued_at: 2024-10-05 11:34:13.055530+00:00. externally triggered: True> failed
Dag run  in failure state
Dag information:talend_job_dag_Sync_Mysql_to_DWh_job Run id: manual__2024-10-05T11:34:13.003458+00:00 external trigger: True
Failed with message: task_failure
[2024-10-05T18:39:23.383+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-05 11:34:13.003458+00:00, run_id=manual__2024-10-05T11:34:13.003458+00:00, run_start_date=2024-10-05 11:34:14.796618+00:00, run_end_date=2024-10-05 11:39:23.382824+00:00, run_duration=308.586206, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-10-04 03:45:00+00:00, data_interval_end=2024-10-05 03:45:00+00:00, dag_hash=c7da5c580ea09a58b2ba6ef98a64bdc7
[2024-10-05T18:39:41.581+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2024-10-05 11:41:54.862257+00:00 hash info: c7da5c580ea09a58b2ba6ef98a64bdc7
[2024-10-05T18:41:55.978+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:41:54.822213+00:00 [scheduled]>
[2024-10-05T18:41:55.980+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T18:41:55.981+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:41:54.822213+00:00 [scheduled]>
[2024-10-05T18:41:55.986+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:41:54.822213+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T18:41:55.988+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T11:41:54.822213+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T18:41:55.989+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T11:41:54.822213+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T18:41:55.995+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T11:41:54.822213+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T18:41:57.748+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T18:41:57.760+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T18:41:57.810+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: sh /opt/Sync_Mysql_to_DWH_Job/Sync_Mysql_to_DWH_temp/Sync_Mysql_to_DWH_temp_run.sh
[2024-10-05T18:41:57.831+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T18:41:58.229+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T18:41:58.230+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T18:41:58.243+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T18:41:58.274+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:41:54.822213+00:00 [queued]> on host linux-ip-147
[2024-10-05T18:41:59.262+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T11:41:54.822213+00:00', try_number=1, map_index=-1)
[2024-10-05T18:41:59.270+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T11:41:54.822213+00:00, map_index=-1, run_start_date=2024-10-05 11:41:58.352955+00:00, run_end_date=2024-10-05 11:41:58.604778+00:00, run_duration=0.251823, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=9, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 11:41:55.983265+00:00, queued_by_job_id=1, pid=246780
[2024-10-05T18:44:42.099+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T18:46:59.132+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:41:54.822213+00:00 [scheduled]>
[2024-10-05T18:46:59.134+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T18:46:59.135+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:41:54.822213+00:00 [scheduled]>
[2024-10-05T18:46:59.139+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:41:54.822213+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T18:46:59.140+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T11:41:54.822213+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T18:46:59.141+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T11:41:54.822213+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T18:46:59.147+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T11:41:54.822213+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T18:47:00.712+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T18:47:00.723+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T18:47:00.769+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: sh /opt/Sync_Mysql_to_DWH_Job/Sync_Mysql_to_DWH_temp/Sync_Mysql_to_DWH_temp_run.sh
[2024-10-05T18:47:00.791+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T18:47:01.196+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T18:47:01.197+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T18:47:01.210+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T18:47:01.240+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:41:54.822213+00:00 [queued]> on host linux-ip-147
[2024-10-05T18:47:03.679+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T11:41:54.822213+00:00', try_number=2, map_index=-1)
[2024-10-05T18:47:03.688+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T11:41:54.822213+00:00, map_index=-1, run_start_date=2024-10-05 11:47:01.289269+00:00, run_end_date=2024-10-05 11:47:01.553465+00:00, run_duration=0.264196, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=10, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 11:46:59.137239+00:00, queued_by_job_id=1, pid=247067
[2024-10-05T18:47:04.169+0700] {dagrun.py:823} ERROR - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-05 11:41:54.822213+00:00: manual__2024-10-05T11:41:54.822213+00:00, state:running, queued_at: 2024-10-05 11:41:54.862257+00:00. externally triggered: True> failed
Dag run  in failure state
Dag information:talend_job_dag_Sync_Mysql_to_DWh_job Run id: manual__2024-10-05T11:41:54.822213+00:00 external trigger: True
Failed with message: task_failure
[2024-10-05T18:47:04.170+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-05 11:41:54.822213+00:00, run_id=manual__2024-10-05T11:41:54.822213+00:00, run_start_date=2024-10-05 11:41:55.920774+00:00, run_end_date=2024-10-05 11:47:04.170458+00:00, run_duration=308.249684, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-10-04 03:45:00+00:00, data_interval_end=2024-10-05 03:45:00+00:00, dag_hash=4cfab8e68d9e56e365fb59c05e7c374c
Dag run  in running state
Dag information Queued at: 2024-10-05 11:48:12.253675+00:00 hash info: 8d830153bf2d9fd17b2c29e774d8cbe4
[2024-10-05T18:48:13.803+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:48:12.210254+00:00 [scheduled]>
[2024-10-05T18:48:13.805+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T18:48:13.806+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:48:12.210254+00:00 [scheduled]>
[2024-10-05T18:48:13.810+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:48:12.210254+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T18:48:13.811+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T11:48:12.210254+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T18:48:13.812+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T11:48:12.210254+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T18:48:13.818+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T11:48:12.210254+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T18:48:15.594+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T18:48:15.607+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T18:48:15.656+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: sh /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T18:48:15.679+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T18:48:16.112+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T18:48:16.113+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T18:48:16.127+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T18:48:16.159+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:48:12.210254+00:00 [queued]> on host linux-ip-147
[2024-10-05T18:48:17.086+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T11:48:12.210254+00:00', try_number=1, map_index=-1)
[2024-10-05T18:48:17.093+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T11:48:12.210254+00:00, map_index=-1, run_start_date=2024-10-05 11:48:16.213830+00:00, run_end_date=2024-10-05 11:48:16.466800+00:00, run_duration=0.25297, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=11, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 11:48:13.808297+00:00, queued_by_job_id=1, pid=247135
[2024-10-05T18:49:42.542+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T18:53:16.979+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:48:12.210254+00:00 [scheduled]>
[2024-10-05T18:53:16.982+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T18:53:16.983+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:48:12.210254+00:00 [scheduled]>
[2024-10-05T18:53:16.987+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:48:12.210254+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T18:53:16.989+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T11:48:12.210254+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T18:53:16.990+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T11:48:12.210254+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T18:53:16.996+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T11:48:12.210254+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T18:53:19.101+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T18:53:19.117+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T18:53:19.174+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: sh /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T18:53:19.199+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T18:53:19.672+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T18:53:19.673+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T18:53:19.686+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T18:53:19.720+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T11:48:12.210254+00:00 [queued]> on host linux-ip-147
[2024-10-05T18:53:21.652+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T11:48:12.210254+00:00', try_number=2, map_index=-1)
[2024-10-05T18:53:21.663+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T11:48:12.210254+00:00, map_index=-1, run_start_date=2024-10-05 11:53:19.771417+00:00, run_end_date=2024-10-05 11:53:19.927781+00:00, run_duration=0.156364, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=12, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 11:53:16.984909+00:00, queued_by_job_id=1, pid=247403
[2024-10-05T18:53:21.999+0700] {dagrun.py:823} ERROR - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-05 11:48:12.210254+00:00: manual__2024-10-05T11:48:12.210254+00:00, state:running, queued_at: 2024-10-05 11:48:12.253675+00:00. externally triggered: True> failed
Dag run  in failure state
Dag information:talend_job_dag_Sync_Mysql_to_DWh_job Run id: manual__2024-10-05T11:48:12.210254+00:00 external trigger: True
Failed with message: task_failure
[2024-10-05T18:53:22.001+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-05 11:48:12.210254+00:00, run_id=manual__2024-10-05T11:48:12.210254+00:00, run_start_date=2024-10-05 11:48:13.749861+00:00, run_end_date=2024-10-05 11:53:22.000887+00:00, run_duration=308.251026, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-10-04 03:45:00+00:00, data_interval_end=2024-10-05 03:45:00+00:00, dag_hash=8d830153bf2d9fd17b2c29e774d8cbe4
[2024-10-05T18:54:43.144+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T18:59:43.546+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2024-10-05 12:03:44.386203+00:00 hash info: 8d830153bf2d9fd17b2c29e774d8cbe4
[2024-10-05T19:03:46.260+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:03:44.352750+00:00 [scheduled]>
[2024-10-05T19:03:46.262+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T19:03:46.263+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:03:44.352750+00:00 [scheduled]>
[2024-10-05T19:03:46.268+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:03:44.352750+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T19:03:46.269+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T12:03:44.352750+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T19:03:46.270+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T12:03:44.352750+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T19:03:46.276+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T12:03:44.352750+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T19:03:48.454+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T19:03:48.467+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T19:03:48.515+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: sh /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T19:03:48.543+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T19:03:48.995+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T19:03:48.996+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T19:03:49.011+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T19:03:49.045+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:03:44.352750+00:00 [queued]> on host linux-ip-147
[2024-10-05T19:03:49.964+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T12:03:44.352750+00:00', try_number=1, map_index=-1)
[2024-10-05T19:03:49.971+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T12:03:44.352750+00:00, map_index=-1, run_start_date=2024-10-05 12:03:49.092636+00:00, run_end_date=2024-10-05 12:03:49.280676+00:00, run_duration=0.18804, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=13, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 12:03:46.265991+00:00, queued_by_job_id=1, pid=248009
[2024-10-05T19:04:43.973+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T19:08:49.437+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:03:44.352750+00:00 [scheduled]>
[2024-10-05T19:08:49.439+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T19:08:49.440+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:03:44.352750+00:00 [scheduled]>
[2024-10-05T19:08:49.444+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:03:44.352750+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T19:08:49.445+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T12:03:44.352750+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T19:08:49.446+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T12:03:44.352750+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T19:08:49.452+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T12:03:44.352750+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T19:08:51.592+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T19:08:51.627+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T19:08:51.757+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: sh /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T19:08:51.795+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T19:08:52.360+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T19:08:52.361+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T19:08:52.378+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T19:08:52.416+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:03:44.352750+00:00 [queued]> on host linux-ip-147
[2024-10-05T19:08:54.566+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T12:03:44.352750+00:00', try_number=2, map_index=-1)
[2024-10-05T19:08:54.579+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T12:03:44.352750+00:00, map_index=-1, run_start_date=2024-10-05 12:08:52.480656+00:00, run_end_date=2024-10-05 12:08:52.757964+00:00, run_duration=0.277308, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=14, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 12:08:49.442216+00:00, queued_by_job_id=1, pid=248288
[2024-10-05T19:08:54.904+0700] {dagrun.py:823} ERROR - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-05 12:03:44.352750+00:00: manual__2024-10-05T12:03:44.352750+00:00, state:running, queued_at: 2024-10-05 12:03:44.386203+00:00. externally triggered: True> failed
Dag run  in failure state
Dag information:talend_job_dag_Sync_Mysql_to_DWh_job Run id: manual__2024-10-05T12:03:44.352750+00:00 external trigger: True
Failed with message: task_failure
[2024-10-05T19:08:54.906+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-05 12:03:44.352750+00:00, run_id=manual__2024-10-05T12:03:44.352750+00:00, run_start_date=2024-10-05 12:03:46.201180+00:00, run_end_date=2024-10-05 12:08:54.906189+00:00, run_duration=308.705009, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-10-04 03:45:00+00:00, data_interval_end=2024-10-05 03:45:00+00:00, dag_hash=8d830153bf2d9fd17b2c29e774d8cbe4
[2024-10-05T19:09:44.176+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2024-10-05 12:10:59.756062+00:00 hash info: 8d830153bf2d9fd17b2c29e774d8cbe4
[2024-10-05T19:11:00.609+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:10:59.712479+00:00 [scheduled]>
[2024-10-05T19:11:00.610+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T19:11:00.611+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:10:59.712479+00:00 [scheduled]>
[2024-10-05T19:11:00.614+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:10:59.712479+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T19:11:00.615+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T12:10:59.712479+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T19:11:00.616+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T12:10:59.712479+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T19:11:00.621+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T12:10:59.712479+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T19:11:02.456+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T19:11:02.470+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T19:11:02.530+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: sh /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T19:11:02.553+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T19:11:02.999+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T19:11:03.000+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T19:11:03.015+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T19:11:03.048+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:10:59.712479+00:00 [queued]> on host linux-ip-147
[2024-10-05T19:11:04.063+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T12:10:59.712479+00:00', try_number=1, map_index=-1)
[2024-10-05T19:11:04.073+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T12:10:59.712479+00:00, map_index=-1, run_start_date=2024-10-05 12:11:03.137493+00:00, run_end_date=2024-10-05 12:11:03.427188+00:00, run_duration=0.289695, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=15, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 12:11:00.612839+00:00, queued_by_job_id=1, pid=248409
[2024-10-05T19:14:44.457+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T19:16:03.930+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:10:59.712479+00:00 [scheduled]>
[2024-10-05T19:16:03.933+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T19:16:03.934+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:10:59.712479+00:00 [scheduled]>
[2024-10-05T19:16:03.938+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:10:59.712479+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T19:16:03.939+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T12:10:59.712479+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T19:16:03.940+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T12:10:59.712479+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T19:16:03.947+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T12:10:59.712479+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T19:16:05.787+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T19:16:05.799+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T19:16:05.846+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: sh /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T19:16:05.867+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T19:16:06.281+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T19:16:06.282+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T19:16:06.295+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T19:16:06.325+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:10:59.712479+00:00 [queued]> on host linux-ip-147
[2024-10-05T19:16:08.194+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T12:10:59.712479+00:00', try_number=2, map_index=-1)
[2024-10-05T19:16:08.201+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T12:10:59.712479+00:00, map_index=-1, run_start_date=2024-10-05 12:16:06.372376+00:00, run_end_date=2024-10-05 12:16:06.567485+00:00, run_duration=0.195109, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=16, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 12:16:03.935903+00:00, queued_by_job_id=1, pid=248689
[2024-10-05T19:16:08.599+0700] {dagrun.py:823} ERROR - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-05 12:10:59.712479+00:00: manual__2024-10-05T12:10:59.712479+00:00, state:running, queued_at: 2024-10-05 12:10:59.756062+00:00. externally triggered: True> failed
Dag run  in failure state
Dag information:talend_job_dag_Sync_Mysql_to_DWh_job Run id: manual__2024-10-05T12:10:59.712479+00:00 external trigger: True
Failed with message: task_failure
[2024-10-05T19:16:08.601+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-05 12:10:59.712479+00:00, run_id=manual__2024-10-05T12:10:59.712479+00:00, run_start_date=2024-10-05 12:11:00.561176+00:00, run_end_date=2024-10-05 12:16:08.601015+00:00, run_duration=308.039839, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-10-04 03:45:00+00:00, data_interval_end=2024-10-05 03:45:00+00:00, dag_hash=8d830153bf2d9fd17b2c29e774d8cbe4
[2024-10-05T19:19:44.770+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2024-10-05 12:22:43.547786+00:00 hash info: 12e563ad653c84b6471ad16c229706a1
[2024-10-05T19:22:44.127+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:22:43.506551+00:00 [scheduled]>
[2024-10-05T19:22:44.129+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T19:22:44.130+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:22:43.506551+00:00 [scheduled]>
[2024-10-05T19:22:44.134+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:22:43.506551+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T19:22:44.136+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T12:22:43.506551+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T19:22:44.137+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T12:22:43.506551+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T19:22:44.143+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T12:22:43.506551+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T19:22:46.207+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T19:22:46.219+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T19:22:46.268+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: bash /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T19:22:46.289+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T19:22:46.696+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T19:22:46.696+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T19:22:46.709+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T19:22:46.739+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:22:43.506551+00:00 [queued]> on host linux-ip-147
[2024-10-05T19:22:47.495+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T12:22:43.506551+00:00', try_number=1, map_index=-1)
[2024-10-05T19:22:47.505+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T12:22:43.506551+00:00, map_index=-1, run_start_date=2024-10-05 12:22:46.785504+00:00, run_end_date=2024-10-05 12:22:46.918335+00:00, run_duration=0.132831, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=17, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 12:22:44.132221+00:00, queued_by_job_id=1, pid=249085
[2024-10-05T19:24:45.006+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T19:27:47.728+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:22:43.506551+00:00 [scheduled]>
[2024-10-05T19:27:47.730+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T19:27:47.731+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:22:43.506551+00:00 [scheduled]>
[2024-10-05T19:27:47.735+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:22:43.506551+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T19:27:47.737+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T12:22:43.506551+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T19:27:47.738+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T12:22:43.506551+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T19:27:47.744+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T12:22:43.506551+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T19:27:49.565+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T19:27:49.579+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T19:27:49.630+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: bash /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T19:27:49.652+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T19:27:50.072+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T19:27:50.073+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T19:27:50.086+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T19:27:50.118+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:22:43.506551+00:00 [queued]> on host linux-ip-147
[2024-10-05T19:27:52.017+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T12:22:43.506551+00:00', try_number=2, map_index=-1)
[2024-10-05T19:27:52.027+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T12:22:43.506551+00:00, map_index=-1, run_start_date=2024-10-05 12:27:50.166404+00:00, run_end_date=2024-10-05 12:27:50.286990+00:00, run_duration=0.120586, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=18, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 12:27:47.733645+00:00, queued_by_job_id=1, pid=249403
[2024-10-05T19:27:52.438+0700] {dagrun.py:823} ERROR - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-05 12:22:43.506551+00:00: manual__2024-10-05T12:22:43.506551+00:00, state:running, queued_at: 2024-10-05 12:22:43.547786+00:00. externally triggered: True> failed
Dag run  in failure state
Dag information:talend_job_dag_Sync_Mysql_to_DWh_job Run id: manual__2024-10-05T12:22:43.506551+00:00 external trigger: True
Failed with message: task_failure
[2024-10-05T19:27:52.440+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-05 12:22:43.506551+00:00, run_id=manual__2024-10-05T12:22:43.506551+00:00, run_start_date=2024-10-05 12:22:44.068758+00:00, run_end_date=2024-10-05 12:27:52.440018+00:00, run_duration=308.37126, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-10-04 03:45:00+00:00, data_interval_end=2024-10-05 03:45:00+00:00, dag_hash=12e563ad653c84b6471ad16c229706a1
Dag run  in running state
Dag information Queued at: 2024-10-05 12:28:11.847264+00:00 hash info: 12e563ad653c84b6471ad16c229706a1
[2024-10-05T19:28:12.970+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:28:11.819726+00:00 [scheduled]>
[2024-10-05T19:28:12.972+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T19:28:12.973+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:28:11.819726+00:00 [scheduled]>
[2024-10-05T19:28:12.977+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:28:11.819726+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T19:28:12.978+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T12:28:11.819726+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T19:28:12.979+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T12:28:11.819726+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T19:28:12.986+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T12:28:11.819726+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T19:28:14.899+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T19:28:14.926+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T19:28:15.033+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: sh /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T19:28:15.066+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T19:28:15.615+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T19:28:15.617+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T19:28:15.649+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T19:28:15.703+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:28:11.819726+00:00 [queued]> on host linux-ip-147
[2024-10-05T19:28:16.537+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T12:28:11.819726+00:00', try_number=1, map_index=-1)
[2024-10-05T19:28:16.561+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T12:28:11.819726+00:00, map_index=-1, run_start_date=2024-10-05 12:28:15.764905+00:00, run_end_date=2024-10-05 12:28:15.943765+00:00, run_duration=0.17886, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=19, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 12:28:12.974988+00:00, queued_by_job_id=1, pid=249426
[2024-10-05T19:29:45.317+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T19:33:16.254+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:28:11.819726+00:00 [scheduled]>
[2024-10-05T19:33:16.256+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T19:33:16.257+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:28:11.819726+00:00 [scheduled]>
[2024-10-05T19:33:16.260+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:28:11.819726+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T19:33:16.262+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T12:28:11.819726+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T19:33:16.263+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T12:28:11.819726+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T19:33:16.270+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T12:28:11.819726+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T19:33:18.358+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T19:33:18.409+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T19:33:18.547+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: sh /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T19:33:18.584+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T19:33:19.101+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T19:33:19.102+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T19:33:19.119+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T19:33:19.157+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:28:11.819726+00:00 [queued]> on host linux-ip-147
[2024-10-05T19:33:21.129+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T12:28:11.819726+00:00', try_number=2, map_index=-1)
[2024-10-05T19:33:21.143+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T12:28:11.819726+00:00, map_index=-1, run_start_date=2024-10-05 12:33:19.216424+00:00, run_end_date=2024-10-05 12:33:19.402544+00:00, run_duration=0.18612, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=20, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 12:33:16.258780+00:00, queued_by_job_id=1, pid=249721
[2024-10-05T19:33:21.487+0700] {dagrun.py:823} ERROR - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-05 12:28:11.819726+00:00: manual__2024-10-05T12:28:11.819726+00:00, state:running, queued_at: 2024-10-05 12:28:11.847264+00:00. externally triggered: True> failed
Dag run  in failure state
Dag information:talend_job_dag_Sync_Mysql_to_DWh_job Run id: manual__2024-10-05T12:28:11.819726+00:00 external trigger: True
Failed with message: task_failure
[2024-10-05T19:33:21.489+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-05 12:28:11.819726+00:00, run_id=manual__2024-10-05T12:28:11.819726+00:00, run_start_date=2024-10-05 12:28:12.908665+00:00, run_end_date=2024-10-05 12:33:21.488958+00:00, run_duration=308.580293, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-10-04 03:45:00+00:00, data_interval_end=2024-10-05 03:45:00+00:00, dag_hash=eac90e46a92b5b240092ed893009e51b
[2024-10-05T19:34:45.329+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2024-10-05 12:37:56.503835+00:00 hash info: eac90e46a92b5b240092ed893009e51b
[2024-10-05T19:37:58.089+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:37:56.473525+00:00 [scheduled]>
[2024-10-05T19:37:58.091+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T19:37:58.092+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:37:56.473525+00:00 [scheduled]>
[2024-10-05T19:37:58.096+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:37:56.473525+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T19:37:58.097+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T12:37:56.473525+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T19:37:58.098+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T12:37:56.473525+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T19:37:58.104+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T12:37:56.473525+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T19:37:59.859+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T19:37:59.883+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T19:37:59.987+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: sh /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T19:38:00.046+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T19:38:00.595+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T19:38:00.597+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T19:38:00.632+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T19:38:00.698+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:37:56.473525+00:00 [queued]> on host linux-ip-147
[2024-10-05T19:38:01.732+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T12:37:56.473525+00:00', try_number=1, map_index=-1)
[2024-10-05T19:38:01.747+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T12:37:56.473525+00:00, map_index=-1, run_start_date=2024-10-05 12:38:00.758698+00:00, run_end_date=2024-10-05 12:38:00.991685+00:00, run_duration=0.232987, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=21, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 12:37:58.094355+00:00, queued_by_job_id=1, pid=249999
[2024-10-05T19:39:45.765+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T19:43:01.347+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:37:56.473525+00:00 [scheduled]>
[2024-10-05T19:43:01.350+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T19:43:01.350+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:37:56.473525+00:00 [scheduled]>
[2024-10-05T19:43:01.354+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:37:56.473525+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T19:43:01.356+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T12:37:56.473525+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T19:43:01.356+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T12:37:56.473525+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T19:43:01.363+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T12:37:56.473525+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T19:43:03.089+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T19:43:03.117+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T19:43:03.218+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: sh /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T19:43:03.262+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T19:43:03.770+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T19:43:03.770+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T19:43:03.785+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T19:43:03.820+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:37:56.473525+00:00 [queued]> on host linux-ip-147
[2024-10-05T19:43:05.818+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T12:37:56.473525+00:00', try_number=2, map_index=-1)
[2024-10-05T19:43:05.828+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T12:37:56.473525+00:00, map_index=-1, run_start_date=2024-10-05 12:43:03.874928+00:00, run_end_date=2024-10-05 12:43:04.126399+00:00, run_duration=0.251471, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=22, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 12:43:01.352461+00:00, queued_by_job_id=1, pid=250284
[2024-10-05T19:43:06.221+0700] {dagrun.py:823} ERROR - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-05 12:37:56.473525+00:00: manual__2024-10-05T12:37:56.473525+00:00, state:running, queued_at: 2024-10-05 12:37:56.503835+00:00. externally triggered: True> failed
Dag run  in failure state
Dag information:talend_job_dag_Sync_Mysql_to_DWh_job Run id: manual__2024-10-05T12:37:56.473525+00:00 external trigger: True
Failed with message: task_failure
[2024-10-05T19:43:06.222+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-05 12:37:56.473525+00:00, run_id=manual__2024-10-05T12:37:56.473525+00:00, run_start_date=2024-10-05 12:37:58.034421+00:00, run_end_date=2024-10-05 12:43:06.222496+00:00, run_duration=308.188075, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-10-04 03:45:00+00:00, data_interval_end=2024-10-05 03:45:00+00:00, dag_hash=eac90e46a92b5b240092ed893009e51b
Dag run  in running state
Dag information Queued at: 2024-10-05 12:43:51.423448+00:00 hash info: eac90e46a92b5b240092ed893009e51b
[2024-10-05T19:43:52.794+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:43:51+00:00 [scheduled]>
[2024-10-05T19:43:52.797+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T19:43:52.799+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:43:51+00:00 [scheduled]>
[2024-10-05T19:43:52.805+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:43:51+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T19:43:52.808+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T12:43:51+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T19:43:52.809+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T12:43:51+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T19:43:52.818+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T12:43:51+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T19:43:54.690+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T19:43:54.701+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T19:43:54.747+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: sh /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T19:43:54.768+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T19:43:55.164+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T19:43:55.164+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T19:43:55.177+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T19:43:55.207+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:43:51+00:00 [queued]> on host linux-ip-147
[2024-10-05T19:43:56.062+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T12:43:51+00:00', try_number=1, map_index=-1)
[2024-10-05T19:43:56.073+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T12:43:51+00:00, map_index=-1, run_start_date=2024-10-05 12:43:55.253912+00:00, run_end_date=2024-10-05 12:43:55.456956+00:00, run_duration=0.203044, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=23, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 12:43:52.801943+00:00, queued_by_job_id=1, pid=250335
[2024-10-05T19:44:46.069+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T19:48:55.495+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:43:51+00:00 [scheduled]>
[2024-10-05T19:48:55.497+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T19:48:55.498+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:43:51+00:00 [scheduled]>
[2024-10-05T19:48:55.501+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:43:51+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T19:48:55.503+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T12:43:51+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T19:48:55.504+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T12:43:51+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T19:48:55.509+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T12:43:51+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T19:48:57.196+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T19:48:57.208+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T19:48:57.254+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: sh /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T19:48:57.274+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T19:48:57.675+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T19:48:57.676+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T19:48:57.689+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T19:48:57.719+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T12:43:51+00:00 [queued]> on host linux-ip-147
[2024-10-05T19:48:59.520+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T12:43:51+00:00', try_number=2, map_index=-1)
[2024-10-05T19:48:59.531+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T12:43:51+00:00, map_index=-1, run_start_date=2024-10-05 12:48:57.766907+00:00, run_end_date=2024-10-05 12:48:57.971180+00:00, run_duration=0.204273, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=24, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 12:48:55.499727+00:00, queued_by_job_id=1, pid=250621
[2024-10-05T19:48:59.848+0700] {dagrun.py:823} ERROR - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-05 12:43:51+00:00: manual__2024-10-05T12:43:51+00:00, state:running, queued_at: 2024-10-05 12:43:51.423448+00:00. externally triggered: True> failed
Dag run  in failure state
Dag information:talend_job_dag_Sync_Mysql_to_DWh_job Run id: manual__2024-10-05T12:43:51+00:00 external trigger: True
Failed with message: task_failure
[2024-10-05T19:48:59.850+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-05 12:43:51+00:00, run_id=manual__2024-10-05T12:43:51+00:00, run_start_date=2024-10-05 12:43:52.726635+00:00, run_end_date=2024-10-05 12:48:59.850430+00:00, run_duration=307.123795, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-10-04 03:45:00+00:00, data_interval_end=2024-10-05 03:45:00+00:00, dag_hash=eac90e46a92b5b240092ed893009e51b
[2024-10-05T19:49:46.084+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T19:54:46.468+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T19:59:46.615+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T20:04:46.817+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2024-10-05 13:06:44.323112+00:00 hash info: eac90e46a92b5b240092ed893009e51b
[2024-10-05T20:06:45.248+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:06:44.291507+00:00 [scheduled]>
[2024-10-05T20:06:45.249+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T20:06:45.250+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:06:44.291507+00:00 [scheduled]>
[2024-10-05T20:06:45.253+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:06:44.291507+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T20:06:45.253+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T13:06:44.291507+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T20:06:45.254+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T13:06:44.291507+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T20:06:45.258+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T13:06:44.291507+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T20:06:47.229+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T20:06:47.241+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T20:06:47.288+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: sh /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T20:06:47.309+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T20:06:47.711+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T20:06:47.712+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T20:06:47.725+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T20:06:47.757+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:06:44.291507+00:00 [queued]> on host linux-ip-147
[2024-10-05T20:06:48.674+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T13:06:44.291507+00:00', try_number=1, map_index=-1)
[2024-10-05T20:06:48.686+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T13:06:44.291507+00:00, map_index=-1, run_start_date=2024-10-05 13:06:47.805351+00:00, run_end_date=2024-10-05 13:06:47.967353+00:00, run_duration=0.162002, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=25, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 13:06:45.251316+00:00, queued_by_job_id=1, pid=251707
[2024-10-05T20:09:47.134+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T20:11:48.690+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:06:44.291507+00:00 [scheduled]>
[2024-10-05T20:11:48.693+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T20:11:48.693+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:06:44.291507+00:00 [scheduled]>
[2024-10-05T20:11:48.697+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:06:44.291507+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T20:11:48.699+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T13:06:44.291507+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T20:11:48.700+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T13:06:44.291507+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T20:11:48.706+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T13:06:44.291507+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T20:11:50.692+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T20:11:50.705+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T20:11:50.754+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: sh /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T20:11:50.776+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T20:11:51.197+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T20:11:51.198+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T20:11:51.212+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T20:11:51.248+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:06:44.291507+00:00 [queued]> on host linux-ip-147
[2024-10-05T20:11:53.218+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T13:06:44.291507+00:00', try_number=2, map_index=-1)
[2024-10-05T20:11:53.226+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T13:06:44.291507+00:00, map_index=-1, run_start_date=2024-10-05 13:11:51.299826+00:00, run_end_date=2024-10-05 13:11:51.527742+00:00, run_duration=0.227916, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=26, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 13:11:48.695510+00:00, queued_by_job_id=1, pid=251985
[2024-10-05T20:11:53.637+0700] {dagrun.py:823} ERROR - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-05 13:06:44.291507+00:00: manual__2024-10-05T13:06:44.291507+00:00, state:running, queued_at: 2024-10-05 13:06:44.323112+00:00. externally triggered: True> failed
Dag run  in failure state
Dag information:talend_job_dag_Sync_Mysql_to_DWh_job Run id: manual__2024-10-05T13:06:44.291507+00:00 external trigger: True
Failed with message: task_failure
[2024-10-05T20:11:53.638+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-05 13:06:44.291507+00:00, run_id=manual__2024-10-05T13:06:44.291507+00:00, run_start_date=2024-10-05 13:06:45.203206+00:00, run_end_date=2024-10-05 13:11:53.638350+00:00, run_duration=308.435144, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-10-04 03:45:00+00:00, data_interval_end=2024-10-05 03:45:00+00:00, dag_hash=eac90e46a92b5b240092ed893009e51b
Dag run  in running state
Dag information Queued at: 2024-10-05 13:14:04.622881+00:00 hash info: eac90e46a92b5b240092ed893009e51b
[2024-10-05T20:14:05.722+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:14:04.591326+00:00 [scheduled]>
[2024-10-05T20:14:05.725+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T20:14:05.725+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:14:04.591326+00:00 [scheduled]>
[2024-10-05T20:14:05.729+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:14:04.591326+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T20:14:05.730+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T13:14:04.591326+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T20:14:05.731+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T13:14:04.591326+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T20:14:05.738+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T13:14:04.591326+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T20:14:07.595+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T20:14:07.608+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T20:14:07.659+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: sh /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T20:14:07.681+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T20:14:08.125+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T20:14:08.126+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T20:14:08.140+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T20:14:08.173+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:14:04.591326+00:00 [queued]> on host linux-ip-147
[2024-10-05T20:14:09.136+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T13:14:04.591326+00:00', try_number=1, map_index=-1)
[2024-10-05T20:14:09.143+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T13:14:04.591326+00:00, map_index=-1, run_start_date=2024-10-05 13:14:08.225664+00:00, run_end_date=2024-10-05 13:14:08.500970+00:00, run_duration=0.275306, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=27, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 13:14:05.727335+00:00, queued_by_job_id=1, pid=252129
[2024-10-05T20:14:47.239+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T20:19:09.050+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:14:04.591326+00:00 [scheduled]>
[2024-10-05T20:19:09.052+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T20:19:09.053+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:14:04.591326+00:00 [scheduled]>
[2024-10-05T20:19:09.057+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:14:04.591326+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T20:19:09.059+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T13:14:04.591326+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T20:19:09.060+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T13:14:04.591326+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T20:19:09.068+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T13:14:04.591326+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T20:19:10.841+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T20:19:10.854+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T20:19:10.902+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: sh /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T20:19:10.923+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T20:19:11.340+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T20:19:11.341+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T20:19:11.354+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T20:19:11.388+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:14:04.591326+00:00 [queued]> on host linux-ip-147
[2024-10-05T20:19:13.111+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T13:14:04.591326+00:00', try_number=2, map_index=-1)
[2024-10-05T20:19:13.118+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T13:14:04.591326+00:00, map_index=-1, run_start_date=2024-10-05 13:19:11.435477+00:00, run_end_date=2024-10-05 13:19:11.612857+00:00, run_duration=0.17738, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=28, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 13:19:09.055284+00:00, queued_by_job_id=1, pid=252393
[2024-10-05T20:19:13.310+0700] {dagrun.py:823} ERROR - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-05 13:14:04.591326+00:00: manual__2024-10-05T13:14:04.591326+00:00, state:running, queued_at: 2024-10-05 13:14:04.622881+00:00. externally triggered: True> failed
Dag run  in failure state
Dag information:talend_job_dag_Sync_Mysql_to_DWh_job Run id: manual__2024-10-05T13:14:04.591326+00:00 external trigger: True
Failed with message: task_failure
[2024-10-05T20:19:13.312+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-05 13:14:04.591326+00:00, run_id=manual__2024-10-05T13:14:04.591326+00:00, run_start_date=2024-10-05 13:14:05.668249+00:00, run_end_date=2024-10-05 13:19:13.311860+00:00, run_duration=307.643611, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-10-04 03:45:00+00:00, data_interval_end=2024-10-05 03:45:00+00:00, dag_hash=ebaf41cb217210a2ef4d51b338244c26
[2024-10-05T20:19:47.626+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2024-10-05 13:21:46.310232+00:00 hash info: ebaf41cb217210a2ef4d51b338244c26
[2024-10-05T20:21:47.700+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:21:46.276882+00:00 [scheduled]>
[2024-10-05T20:21:47.702+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T20:21:47.703+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:21:46.276882+00:00 [scheduled]>
[2024-10-05T20:21:47.707+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:21:46.276882+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T20:21:47.708+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T13:21:46.276882+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T20:21:47.709+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T13:21:46.276882+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T20:21:47.715+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T13:21:46.276882+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T20:21:49.485+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T20:21:49.497+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T20:21:49.546+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: sh /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T20:21:49.567+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T20:21:49.992+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T20:21:49.992+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T20:21:50.006+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T20:21:50.047+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:21:46.276882+00:00 [queued]> on host linux-ip-147
[2024-10-05T20:21:50.848+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T13:21:46.276882+00:00', try_number=1, map_index=-1)
[2024-10-05T20:21:50.862+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T13:21:46.276882+00:00, map_index=-1, run_start_date=2024-10-05 13:21:50.095365+00:00, run_end_date=2024-10-05 13:21:50.228655+00:00, run_duration=0.13329, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=29, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 13:21:47.704855+00:00, queued_by_job_id=1, pid=252545
[2024-10-05T20:24:47.665+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T20:26:51.242+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:21:46.276882+00:00 [scheduled]>
[2024-10-05T20:26:51.243+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T20:26:51.244+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:21:46.276882+00:00 [scheduled]>
[2024-10-05T20:26:51.246+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:21:46.276882+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T20:26:51.247+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T13:21:46.276882+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T20:26:51.248+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T13:21:46.276882+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T20:26:51.254+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T13:21:46.276882+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T20:26:52.994+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T20:26:53.006+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T20:26:53.054+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: sh /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T20:26:53.074+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T20:26:53.480+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T20:26:53.480+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T20:26:53.493+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T20:26:53.525+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:21:46.276882+00:00 [queued]> on host linux-ip-147
[2024-10-05T20:26:54.510+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T13:21:46.276882+00:00', try_number=2, map_index=-1)
[2024-10-05T20:26:54.522+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T13:21:46.276882+00:00, map_index=-1, run_start_date=2024-10-05 13:26:53.572563+00:00, run_end_date=2024-10-05 13:26:53.792023+00:00, run_duration=0.21946, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=30, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 13:26:51.245280+00:00, queued_by_job_id=1, pid=252820
[2024-10-05T20:26:54.937+0700] {dagrun.py:823} ERROR - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-05 13:21:46.276882+00:00: manual__2024-10-05T13:21:46.276882+00:00, state:running, queued_at: 2024-10-05 13:21:46.310232+00:00. externally triggered: True> failed
Dag run  in failure state
Dag information:talend_job_dag_Sync_Mysql_to_DWh_job Run id: manual__2024-10-05T13:21:46.276882+00:00 external trigger: True
Failed with message: task_failure
[2024-10-05T20:26:54.941+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-05 13:21:46.276882+00:00, run_id=manual__2024-10-05T13:21:46.276882+00:00, run_start_date=2024-10-05 13:21:47.640603+00:00, run_end_date=2024-10-05 13:26:54.940797+00:00, run_duration=307.300194, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-10-04 03:45:00+00:00, data_interval_end=2024-10-05 03:45:00+00:00, dag_hash=418d078dc29932be0ccc4f0f6490d456
Dag run  in running state
Dag information Queued at: 2024-10-05 13:29:38.229770+00:00 hash info: 418d078dc29932be0ccc4f0f6490d456
[2024-10-05T20:29:39.950+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:29:38.188987+00:00 [scheduled]>
[2024-10-05T20:29:39.952+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T20:29:39.953+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:29:38.188987+00:00 [scheduled]>
[2024-10-05T20:29:39.957+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:29:38.188987+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T20:29:39.958+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T13:29:38.188987+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T20:29:39.959+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T13:29:38.188987+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T20:29:39.964+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T13:29:38.188987+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T20:29:41.838+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T20:29:41.862+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T20:29:41.923+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: sh /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T20:29:41.946+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T20:29:42.500+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T20:29:42.501+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T20:29:42.517+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T20:29:42.553+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:29:38.188987+00:00 [queued]> on host linux-ip-147
[2024-10-05T20:29:43.499+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T13:29:38.188987+00:00', try_number=1, map_index=-1)
[2024-10-05T20:29:43.508+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T13:29:38.188987+00:00, map_index=-1, run_start_date=2024-10-05 13:29:42.630803+00:00, run_end_date=2024-10-05 13:29:42.887298+00:00, run_duration=0.256495, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=31, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 13:29:39.955058+00:00, queued_by_job_id=1, pid=252969
[2024-10-05T20:29:48.094+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T20:34:43.017+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:29:38.188987+00:00 [scheduled]>
[2024-10-05T20:34:43.019+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T20:34:43.020+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:29:38.188987+00:00 [scheduled]>
[2024-10-05T20:34:43.023+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:29:38.188987+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T20:34:43.025+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T13:29:38.188987+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T20:34:43.026+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T13:29:38.188987+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T20:34:43.031+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T13:29:38.188987+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T20:34:44.765+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T20:34:44.776+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T20:34:44.821+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: sh /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T20:34:44.841+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T20:34:45.234+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T20:34:45.235+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T20:34:45.247+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T20:34:45.277+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:29:38.188987+00:00 [queued]> on host linux-ip-147
[2024-10-05T20:34:47.333+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T13:29:38.188987+00:00', try_number=2, map_index=-1)
[2024-10-05T20:34:47.340+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T13:29:38.188987+00:00, map_index=-1, run_start_date=2024-10-05 13:34:45.351940+00:00, run_end_date=2024-10-05 13:34:45.620976+00:00, run_duration=0.269036, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=32, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 13:34:43.021787+00:00, queued_by_job_id=1, pid=253240
[2024-10-05T20:34:47.677+0700] {dagrun.py:823} ERROR - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-05 13:29:38.188987+00:00: manual__2024-10-05T13:29:38.188987+00:00, state:running, queued_at: 2024-10-05 13:29:38.229770+00:00. externally triggered: True> failed
Dag run  in failure state
Dag information:talend_job_dag_Sync_Mysql_to_DWh_job Run id: manual__2024-10-05T13:29:38.188987+00:00 external trigger: True
Failed with message: task_failure
[2024-10-05T20:34:47.679+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-05 13:29:38.188987+00:00, run_id=manual__2024-10-05T13:29:38.188987+00:00, run_start_date=2024-10-05 13:29:39.892593+00:00, run_end_date=2024-10-05 13:34:47.678606+00:00, run_duration=307.786013, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-10-04 03:45:00+00:00, data_interval_end=2024-10-05 03:45:00+00:00, dag_hash=418d078dc29932be0ccc4f0f6490d456
[2024-10-05T20:34:48.291+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T20:39:48.633+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2024-10-05 13:40:28.584756+00:00 hash info: 418d078dc29932be0ccc4f0f6490d456
[2024-10-05T20:40:29.588+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:40:28.551915+00:00 [scheduled]>
[2024-10-05T20:40:29.590+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T20:40:29.591+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:40:28.551915+00:00 [scheduled]>
[2024-10-05T20:40:29.594+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:40:28.551915+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T20:40:29.596+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T13:40:28.551915+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T20:40:29.597+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T13:40:28.551915+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T20:40:29.604+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T13:40:28.551915+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T20:40:31.312+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T20:40:31.324+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T20:40:31.371+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T20:40:31.393+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T20:40:31.802+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T20:40:31.803+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T20:40:31.815+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T20:40:31.846+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:40:28.551915+00:00 [queued]> on host linux-ip-147
[2024-10-05T20:40:32.843+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T13:40:28.551915+00:00', try_number=1, map_index=-1)
[2024-10-05T20:40:32.858+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T13:40:28.551915+00:00, map_index=-1, run_start_date=2024-10-05 13:40:31.920482+00:00, run_end_date=2024-10-05 13:40:32.129669+00:00, run_duration=0.209187, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=33, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 13:40:29.592710+00:00, queued_by_job_id=1, pid=253568
[2024-10-05T20:44:49.074+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T20:45:33.116+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:40:28.551915+00:00 [scheduled]>
[2024-10-05T20:45:33.118+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T20:45:33.118+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:40:28.551915+00:00 [scheduled]>
[2024-10-05T20:45:33.122+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:40:28.551915+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T20:45:33.124+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T13:40:28.551915+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T20:45:33.125+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T13:40:28.551915+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T20:45:33.130+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T13:40:28.551915+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T20:45:34.723+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T20:45:34.735+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T20:45:34.781+0700] {templater.py:94} ERROR - Failed to resolve template field 'bash_command'
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/template/templater.py", line 92, in resolve_template_files
    setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore
  File "/usr/lib/python3/dist-packages/jinja2/loaders.py", line 214, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: /opt/Sync_Mysql_to_DWH_Job/run_Sync_Mysql_to_DWH_job.sh
[2024-10-05T20:45:34.801+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T20:45:35.198+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T20:45:35.199+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T20:45:35.212+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T20:45:35.243+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:40:28.551915+00:00 [queued]> on host linux-ip-147
[2024-10-05T20:45:37.262+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T13:40:28.551915+00:00', try_number=2, map_index=-1)
[2024-10-05T20:45:37.270+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T13:40:28.551915+00:00, map_index=-1, run_start_date=2024-10-05 13:45:35.289202+00:00, run_end_date=2024-10-05 13:45:35.415015+00:00, run_duration=0.125813, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=34, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 13:45:33.120601+00:00, queued_by_job_id=1, pid=253852
[2024-10-05T20:45:37.690+0700] {dagrun.py:823} ERROR - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-05 13:40:28.551915+00:00: manual__2024-10-05T13:40:28.551915+00:00, state:running, queued_at: 2024-10-05 13:40:28.584756+00:00. externally triggered: True> failed
Dag run  in failure state
Dag information:talend_job_dag_Sync_Mysql_to_DWh_job Run id: manual__2024-10-05T13:40:28.551915+00:00 external trigger: True
Failed with message: task_failure
[2024-10-05T20:45:37.692+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-05 13:40:28.551915+00:00, run_id=manual__2024-10-05T13:40:28.551915+00:00, run_start_date=2024-10-05 13:40:29.530151+00:00, run_end_date=2024-10-05 13:45:37.692364+00:00, run_duration=308.162213, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-10-04 03:45:00+00:00, data_interval_end=2024-10-05 03:45:00+00:00, dag_hash=f9ae3e048b04906522325f83eec7cf97
Dag run  in running state
Dag information Queued at: 2024-10-05 13:47:27.500293+00:00 hash info: f9ae3e048b04906522325f83eec7cf97
[2024-10-05T20:47:28.135+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:47:27.468801+00:00 [scheduled]>
[2024-10-05T20:47:28.137+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T20:47:28.138+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:47:27.468801+00:00 [scheduled]>
[2024-10-05T20:47:28.142+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:47:27.468801+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T20:47:28.144+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T13:47:27.468801+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T20:47:28.145+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T13:47:27.468801+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T20:47:28.151+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T13:47:27.468801+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T20:47:29.868+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T20:47:29.892+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T20:47:30.029+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T20:47:30.492+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T20:47:30.493+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T20:47:30.510+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T20:47:30.544+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T13:47:27.468801+00:00 [queued]> on host linux-ip-147
[2024-10-05T20:47:35.635+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T13:47:27.468801+00:00', try_number=1, map_index=-1)
[2024-10-05T20:47:35.647+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T13:47:27.468801+00:00, map_index=-1, run_start_date=2024-10-05 13:47:30.597284+00:00, run_end_date=2024-10-05 13:47:34.904220+00:00, run_duration=4.306936, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=35, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 13:47:28.139857+00:00, queued_by_job_id=1, pid=253960
[2024-10-05T20:47:36.103+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T13:47:27.468801+00:00 [scheduled]>
[2024-10-05T20:47:36.104+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T20:47:36.105+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T13:47:27.468801+00:00 [scheduled]>
[2024-10-05T20:47:36.109+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T13:47:27.468801+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T20:47:36.110+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='notify_success', run_id='manual__2024-10-05T13:47:27.468801+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2024-10-05T20:47:36.111+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'notify_success', 'manual__2024-10-05T13:47:27.468801+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T20:47:36.117+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'notify_success', 'manual__2024-10-05T13:47:27.468801+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T20:47:39.655+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T20:47:39.669+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T20:47:39.789+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T20:47:40.350+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T20:47:40.351+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T20:47:40.367+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T20:47:40.406+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T13:47:27.468801+00:00 [queued]> on host linux-ip-147
[2024-10-05T20:47:42.644+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='notify_success', run_id='manual__2024-10-05T13:47:27.468801+00:00', try_number=1, map_index=-1)
[2024-10-05T20:47:42.656+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=notify_success, run_id=manual__2024-10-05T13:47:27.468801+00:00, map_index=-1, run_start_date=2024-10-05 13:47:40.463711+00:00, run_end_date=2024-10-05 13:47:41.932259+00:00, run_duration=1.468548, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=36, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-10-05 13:47:36.107046+00:00, queued_by_job_id=1, pid=253990
[2024-10-05T20:47:43.058+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-05 13:47:27.468801+00:00: manual__2024-10-05T13:47:27.468801+00:00, state:running, queued_at: 2024-10-05 13:47:27.500293+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-05 13:47:28.075267+00:00 end:2024-10-05 13:47:43.059504+00:00
[2024-10-05T20:47:43.059+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-05 13:47:27.468801+00:00, run_id=manual__2024-10-05T13:47:27.468801+00:00, run_start_date=2024-10-05 13:47:28.075267+00:00, run_end_date=2024-10-05 13:47:43.059504+00:00, run_duration=14.984237, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-04 03:45:00+00:00, data_interval_end=2024-10-05 03:45:00+00:00, dag_hash=f9ae3e048b04906522325f83eec7cf97
[2024-10-05T20:49:49.476+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T20:54:49.798+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T20:59:49.896+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T21:04:50.297+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2024-10-05 14:05:17.342111+00:00 hash info: a9a97e386dbba3789203b2bbef133c22
[2024-10-05T21:05:19.153+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:05:17.313349+00:00 [scheduled]>
[2024-10-05T21:05:19.154+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T21:05:19.155+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:05:17.313349+00:00 [scheduled]>
[2024-10-05T21:05:19.159+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:05:17.313349+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T21:05:19.160+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T14:05:17.313349+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T21:05:19.161+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T14:05:17.313349+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:05:19.166+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T14:05:17.313349+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:05:20.839+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:05:20.850+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T21:05:20.881+0700] {dagbag.py:387} ERROR - Failed to import: /root/airflow/dags/Sync_Mysql_to_DWh_job.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/root/airflow/dags/Sync_Mysql_to_DWh_job.py", line 17, in <module>
    dag = DAG(
TypeError: DAG.__init__() got an unexpected keyword argument 'timezone'
[2024-10-05T21:05:20.914+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:05:21.456+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T21:05:21.457+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:05:21.472+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:05:21.475+0700] {cli.py:243} WARNING - Dag 'talend_job_dag_Sync_Mysql_to_DWh_job' not found in path /root/airflow/dags/Sync_Mysql_to_DWh_job.py; trying path /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T21:05:21.475+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T21:05:21.498+0700] {dagbag.py:387} ERROR - Failed to import: /root/airflow/dags/Sync_Mysql_to_DWh_job.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/root/airflow/dags/Sync_Mysql_to_DWh_job.py", line 17, in <module>
    dag = DAG(
TypeError: DAG.__init__() got an unexpected keyword argument 'timezone'
[2024-10-05T21:05:21.504+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:05:21.710+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T21:05:21.710+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:05:21.724+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:05:21.733+0700] {dagbag.py:387} ERROR - Failed to import: /root/airflow/dags/Sync_Mysql_to_DWh_job.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/root/airflow/dags/Sync_Mysql_to_DWh_job.py", line 17, in <module>
    dag = DAG(
TypeError: DAG.__init__() got an unexpected keyword argument 'timezone'
[2024-10-05T21:05:22.230+0700] {sequential_executor.py:91} ERROR - Failed to execute task Command '['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T14:05:17.313349+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']' returned non-zero exit status 1..
[2024-10-05T21:05:22.232+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T14:05:17.313349+00:00', try_number=1, map_index=-1)
[2024-10-05T21:05:22.243+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T14:05:17.313349+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor=SequentialExecutor(parallelism=32), executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 14:05:19.157061+00:00, queued_by_job_id=1, pid=None
[2024-10-05T21:05:22.245+0700] {scheduler_job_runner.py:907} ERROR - Executor SequentialExecutor(parallelism=32) reported that the task instance <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:05:17.313349+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally
[2024-10-05T21:06:22.902+0700] {manager.py:537} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job is missing and will be deactivated.
[2024-10-05T21:06:22.908+0700] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2024-10-05T21:06:22.916+0700] {manager.py:553} INFO - Deleted DAG talend_job_dag_Sync_Mysql_to_DWh_job in serialized_dag table
[2024-10-05T21:09:50.361+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T21:11:19.990+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:05:17.313349+00:00 [scheduled]>
[2024-10-05T21:11:19.992+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T21:11:19.993+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:05:17.313349+00:00 [scheduled]>
[2024-10-05T21:11:19.997+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:05:17.313349+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T21:11:19.998+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T14:05:17.313349+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T21:11:19.999+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T14:05:17.313349+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:11:20.011+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T14:05:17.313349+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:11:21.966+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:11:21.978+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T21:11:22.043+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:11:22.447+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T21:11:22.448+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:11:22.462+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:11:22.491+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:05:17.313349+00:00 [queued]> on host linux-ip-147
[2024-10-05T21:11:24.513+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T14:05:17.313349+00:00', try_number=2, map_index=-1)
[2024-10-05T21:11:24.526+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T14:05:17.313349+00:00, map_index=-1, run_start_date=2024-10-05 14:11:22.541317+00:00, run_end_date=2024-10-05 14:11:22.713475+00:00, run_duration=0.172158, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=37, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 14:11:19.995022+00:00, queued_by_job_id=1, pid=255407
[2024-10-05T21:11:24.735+0700] {dagrun.py:823} ERROR - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-05 14:05:17.313349+00:00: manual__2024-10-05T14:05:17.313349+00:00, state:running, queued_at: 2024-10-05 14:05:17.342111+00:00. externally triggered: True> failed
Dag run  in failure state
Dag information:talend_job_dag_Sync_Mysql_to_DWh_job Run id: manual__2024-10-05T14:05:17.313349+00:00 external trigger: True
Failed with message: task_failure
[2024-10-05T21:11:24.737+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-05 14:05:17.313349+00:00, run_id=manual__2024-10-05T14:05:17.313349+00:00, run_start_date=2024-10-05 14:05:19.099829+00:00, run_end_date=2024-10-05 14:11:24.737072+00:00, run_duration=365.637243, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-10-04 03:45:00+00:00, data_interval_end=2024-10-05 03:45:00+00:00, dag_hash=18276d457c31d6195b9e45d77967945b
Dag run  in running state
Dag information Queued at: 2024-10-05 14:12:22.149828+00:00 hash info: 18276d457c31d6195b9e45d77967945b
[2024-10-05T21:12:23.265+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:12:22.133065+00:00 [scheduled]>
[2024-10-05T21:12:23.267+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T21:12:23.268+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:12:22.133065+00:00 [scheduled]>
[2024-10-05T21:12:23.272+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:12:22.133065+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T21:12:23.273+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T14:12:22.133065+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T21:12:23.274+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T14:12:22.133065+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:12:23.280+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T14:12:22.133065+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:12:25.182+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:12:25.207+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T21:12:25.330+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:12:25.808+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T21:12:25.809+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:12:25.824+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:12:25.859+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:12:22.133065+00:00 [queued]> on host linux-ip-147
[2024-10-05T21:12:30.439+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T14:12:22.133065+00:00', try_number=1, map_index=-1)
[2024-10-05T21:12:30.455+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T14:12:22.133065+00:00, map_index=-1, run_start_date=2024-10-05 14:12:25.915359+00:00, run_end_date=2024-10-05 14:12:29.714966+00:00, run_duration=3.799607, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=38, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 14:12:23.270116+00:00, queued_by_job_id=1, pid=255467
[2024-10-05T21:12:30.897+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:12:22.133065+00:00 [scheduled]>
[2024-10-05T21:12:30.898+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T21:12:30.899+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:12:22.133065+00:00 [scheduled]>
[2024-10-05T21:12:30.902+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:12:22.133065+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T21:12:30.903+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='notify_success', run_id='manual__2024-10-05T14:12:22.133065+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2024-10-05T21:12:30.904+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'notify_success', 'manual__2024-10-05T14:12:22.133065+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:12:30.910+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'notify_success', 'manual__2024-10-05T14:12:22.133065+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:12:33.894+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:12:33.921+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T21:12:34.065+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:12:34.561+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T21:12:34.562+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:12:34.577+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:12:34.614+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:12:22.133065+00:00 [queued]> on host linux-ip-147
[2024-10-05T21:12:36.559+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='notify_success', run_id='manual__2024-10-05T14:12:22.133065+00:00', try_number=1, map_index=-1)
[2024-10-05T21:12:36.570+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=notify_success, run_id=manual__2024-10-05T14:12:22.133065+00:00, map_index=-1, run_start_date=2024-10-05 14:12:34.669864+00:00, run_end_date=2024-10-05 14:12:35.861679+00:00, run_duration=1.191815, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=39, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-10-05 14:12:30.900753+00:00, queued_by_job_id=1, pid=255493
[2024-10-05T21:12:37.010+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-05 14:12:22.133065+00:00: manual__2024-10-05T14:12:22.133065+00:00, state:running, queued_at: 2024-10-05 14:12:22.149828+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-05 14:12:23.208709+00:00 end:2024-10-05 14:12:37.011756+00:00
[2024-10-05T21:12:37.012+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-05 14:12:22.133065+00:00, run_id=manual__2024-10-05T14:12:22.133065+00:00, run_start_date=2024-10-05 14:12:23.208709+00:00, run_end_date=2024-10-05 14:12:37.011756+00:00, run_duration=13.803047, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-04 08:00:00+00:00, data_interval_end=2024-10-05 08:00:00+00:00, dag_hash=18276d457c31d6195b9e45d77967945b
Dag run  in running state
Dag information Queued at: 2024-10-05 14:12:56.683192+00:00 hash info: 8b51c21f5a98bb2d21994930eb65c561
[2024-10-05T21:12:57.603+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:12:56.655152+00:00 [scheduled]>
[2024-10-05T21:12:57.604+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T21:12:57.605+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:12:56.655152+00:00 [scheduled]>
[2024-10-05T21:12:57.609+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:12:56.655152+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T21:12:57.611+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T14:12:56.655152+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T21:12:57.611+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T14:12:56.655152+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:12:57.617+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T14:12:56.655152+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:12:59.401+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:12:59.413+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T21:12:59.480+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:12:59.915+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T21:12:59.916+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:12:59.930+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:12:59.962+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:12:56.655152+00:00 [queued]> on host linux-ip-147
[2024-10-05T21:13:01.089+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T14:12:56.655152+00:00', try_number=1, map_index=-1)
[2024-10-05T21:13:01.100+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T14:12:56.655152+00:00, map_index=-1, run_start_date=2024-10-05 14:13:00.061872+00:00, run_end_date=2024-10-05 14:13:00.410431+00:00, run_duration=0.348559, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=40, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 14:12:57.607299+00:00, queued_by_job_id=1, pid=255516
[2024-10-05T21:14:50.784+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T21:18:01.712+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:12:56.655152+00:00 [scheduled]>
[2024-10-05T21:18:01.714+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T21:18:01.715+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:12:56.655152+00:00 [scheduled]>
[2024-10-05T21:18:01.719+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:12:56.655152+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T21:18:01.720+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T14:12:56.655152+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T21:18:01.722+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T14:12:56.655152+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:18:01.728+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T14:12:56.655152+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:18:03.641+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:18:03.654+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T21:18:03.727+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:18:04.191+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T21:18:04.192+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:18:04.207+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:18:04.241+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:12:56.655152+00:00 [queued]> on host linux-ip-147
[2024-10-05T21:18:06.524+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T14:12:56.655152+00:00', try_number=2, map_index=-1)
[2024-10-05T21:18:06.534+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T14:12:56.655152+00:00, map_index=-1, run_start_date=2024-10-05 14:18:04.319782+00:00, run_end_date=2024-10-05 14:18:04.627919+00:00, run_duration=0.308137, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=41, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 14:18:01.717225+00:00, queued_by_job_id=1, pid=255799
[2024-10-05T21:18:06.832+0700] {dagrun.py:823} ERROR - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-05 14:12:56.655152+00:00: manual__2024-10-05T14:12:56.655152+00:00, state:running, queued_at: 2024-10-05 14:12:56.683192+00:00. externally triggered: True> failed
Dag run  in failure state
Dag information:talend_job_dag_Sync_Mysql_to_DWh_job Run id: manual__2024-10-05T14:12:56.655152+00:00 external trigger: True
Failed with message: task_failure
[2024-10-05T21:18:06.833+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-05 14:12:56.655152+00:00, run_id=manual__2024-10-05T14:12:56.655152+00:00, run_start_date=2024-10-05 14:12:57.551727+00:00, run_end_date=2024-10-05 14:18:06.833352+00:00, run_duration=309.281625, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-10-04 08:00:00+00:00, data_interval_end=2024-10-05 08:00:00+00:00, dag_hash=18276d457c31d6195b9e45d77967945b
Dag run  in running state
Dag information Queued at: 2024-10-05 14:18:32.415849+00:00 hash info: 18276d457c31d6195b9e45d77967945b
[2024-10-05T21:18:33.053+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:18:32.386943+00:00 [scheduled]>
[2024-10-05T21:18:33.054+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T21:18:33.054+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:18:32.386943+00:00 [scheduled]>
[2024-10-05T21:18:33.056+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:18:32.386943+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T21:18:33.057+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T14:18:32.386943+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T21:18:33.057+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T14:18:32.386943+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:18:33.061+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T14:18:32.386943+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:18:34.763+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:18:34.774+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T21:18:34.841+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:18:35.258+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T21:18:35.259+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:18:35.272+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:18:35.304+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:18:32.386943+00:00 [queued]> on host linux-ip-147
[2024-10-05T21:18:39.824+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T14:18:32.386943+00:00', try_number=1, map_index=-1)
[2024-10-05T21:18:39.834+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T14:18:32.386943+00:00, map_index=-1, run_start_date=2024-10-05 14:18:35.381986+00:00, run_end_date=2024-10-05 14:18:39.144982+00:00, run_duration=3.762996, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=42, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 14:18:33.055158+00:00, queued_by_job_id=1, pid=255827
[2024-10-05T21:18:40.389+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:18:32.386943+00:00 [scheduled]>
[2024-10-05T21:18:40.390+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T21:18:40.391+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:18:32.386943+00:00 [scheduled]>
[2024-10-05T21:18:40.395+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:18:32.386943+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T21:18:40.396+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='notify_success', run_id='manual__2024-10-05T14:18:32.386943+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2024-10-05T21:18:40.397+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'notify_success', 'manual__2024-10-05T14:18:32.386943+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:18:40.407+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'notify_success', 'manual__2024-10-05T14:18:32.386943+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:18:42.655+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:18:42.669+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T21:18:42.743+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:18:43.217+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T21:18:43.218+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:18:43.233+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:18:43.269+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:18:32.386943+00:00 [queued]> on host linux-ip-147
[2024-10-05T21:18:45.339+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='notify_success', run_id='manual__2024-10-05T14:18:32.386943+00:00', try_number=1, map_index=-1)
[2024-10-05T21:18:45.347+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=notify_success, run_id=manual__2024-10-05T14:18:32.386943+00:00, map_index=-1, run_start_date=2024-10-05 14:18:43.326912+00:00, run_end_date=2024-10-05 14:18:44.699348+00:00, run_duration=1.372436, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=43, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-10-05 14:18:40.393052+00:00, queued_by_job_id=1, pid=255854
[2024-10-05T21:18:45.595+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-05 14:18:32.386943+00:00: manual__2024-10-05T14:18:32.386943+00:00, state:running, queued_at: 2024-10-05 14:18:32.415849+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-05 14:18:33.023733+00:00 end:2024-10-05 14:18:45.597130+00:00
[2024-10-05T21:18:45.597+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-05 14:18:32.386943+00:00, run_id=manual__2024-10-05T14:18:32.386943+00:00, run_start_date=2024-10-05 14:18:33.023733+00:00, run_end_date=2024-10-05 14:18:45.597130+00:00, run_duration=12.573397, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-04 08:00:00+00:00, data_interval_end=2024-10-05 08:00:00+00:00, dag_hash=8b51c21f5a98bb2d21994930eb65c561
[2024-10-05T21:19:51.170+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T21:24:51.366+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2024-10-05 14:29:40.595318+00:00 hash info: 2eab5749fefaddd1b80f7b1e084a79ed
[2024-10-05T21:29:41.267+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:29:40.568355+00:00 [scheduled]>
[2024-10-05T21:29:41.268+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T21:29:41.269+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:29:40.568355+00:00 [scheduled]>
[2024-10-05T21:29:41.271+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:29:40.568355+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T21:29:41.271+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T14:29:40.568355+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T21:29:41.272+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T14:29:40.568355+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:29:41.276+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T14:29:40.568355+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:29:42.991+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:29:43.003+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T21:29:43.069+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:29:43.487+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T21:29:43.487+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:29:43.500+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:29:43.530+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:29:40.568355+00:00 [queued]> on host linux-ip-147
[2024-10-05T21:29:49.646+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T14:29:40.568355+00:00', try_number=1, map_index=-1)
[2024-10-05T21:29:49.661+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T14:29:40.568355+00:00, map_index=-1, run_start_date=2024-10-05 14:29:43.609095+00:00, run_end_date=2024-10-05 14:29:48.917365+00:00, run_duration=5.30827, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=44, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 14:29:41.269895+00:00, queued_by_job_id=1, pid=256468
[2024-10-05T21:29:50.127+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:29:40.568355+00:00 [scheduled]>
[2024-10-05T21:29:50.128+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T21:29:50.129+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:29:40.568355+00:00 [scheduled]>
[2024-10-05T21:29:50.133+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:29:40.568355+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T21:29:50.134+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='notify_success', run_id='manual__2024-10-05T14:29:40.568355+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2024-10-05T21:29:50.135+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'notify_success', 'manual__2024-10-05T14:29:40.568355+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:29:50.142+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'notify_success', 'manual__2024-10-05T14:29:40.568355+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:29:52.333+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:29:52.369+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T21:29:52.509+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:29:53.042+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T21:29:53.042+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:29:53.058+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:29:53.097+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:29:40.568355+00:00 [queued]> on host linux-ip-147
[2024-10-05T21:29:54.123+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='notify_success', run_id='manual__2024-10-05T14:29:40.568355+00:00', try_number=1, map_index=-1)
[2024-10-05T21:29:54.139+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=notify_success, run_id=manual__2024-10-05T14:29:40.568355+00:00, map_index=-1, run_start_date=2024-10-05 14:29:53.188580+00:00, run_end_date=2024-10-05 14:29:53.501041+00:00, run_duration=0.312461, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=45, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-10-05 14:29:50.131019+00:00, queued_by_job_id=1, pid=256494
[2024-10-05T21:29:54.153+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2024-10-05 14:30:18.635227+00:00 hash info: 2eab5749fefaddd1b80f7b1e084a79ed
[2024-10-05T21:30:20.471+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:30:18.597436+00:00 [scheduled]>
[2024-10-05T21:30:20.479+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T21:30:20.482+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:30:18.597436+00:00 [scheduled]>
[2024-10-05T21:30:20.497+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:30:18.597436+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T21:30:20.499+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T14:30:18.597436+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T21:30:20.500+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T14:30:18.597436+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:30:20.517+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T14:30:18.597436+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:30:22.344+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:30:22.358+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T21:30:22.437+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:30:22.924+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T21:30:22.925+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:30:22.946+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:30:22.986+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:30:18.597436+00:00 [queued]> on host linux-ip-147
[2024-10-05T21:30:27.748+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T14:30:18.597436+00:00', try_number=1, map_index=-1)
[2024-10-05T21:30:27.762+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T14:30:18.597436+00:00, map_index=-1, run_start_date=2024-10-05 14:30:23.037786+00:00, run_end_date=2024-10-05 14:30:27.024798+00:00, run_duration=3.987012, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=46, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 14:30:20.484250+00:00, queued_by_job_id=1, pid=256522
[2024-10-05T21:30:28.130+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:30:18.597436+00:00 [scheduled]>
[2024-10-05T21:30:28.132+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T21:30:28.133+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:30:18.597436+00:00 [scheduled]>
[2024-10-05T21:30:28.136+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:30:18.597436+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T21:30:28.138+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='notify_success', run_id='manual__2024-10-05T14:30:18.597436+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2024-10-05T21:30:28.138+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'notify_success', 'manual__2024-10-05T14:30:18.597436+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:30:28.145+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'notify_success', 'manual__2024-10-05T14:30:18.597436+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:30:33.020+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:30:33.052+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T21:30:33.241+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:30:33.849+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T21:30:33.850+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:30:33.866+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:30:33.910+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:30:18.597436+00:00 [queued]> on host linux-ip-147
[2024-10-05T21:30:35.078+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='notify_success', run_id='manual__2024-10-05T14:30:18.597436+00:00', try_number=1, map_index=-1)
[2024-10-05T21:30:35.090+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=notify_success, run_id=manual__2024-10-05T14:30:18.597436+00:00, map_index=-1, run_start_date=2024-10-05 14:30:34.008369+00:00, run_end_date=2024-10-05 14:30:34.361957+00:00, run_duration=0.353588, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=47, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-10-05 14:30:28.134548+00:00, queued_by_job_id=1, pid=256548
[2024-10-05T21:34:53.531+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:29:40.568355+00:00 [scheduled]>
[2024-10-05T21:34:53.534+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T21:34:53.535+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:29:40.568355+00:00 [scheduled]>
[2024-10-05T21:34:53.540+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:29:40.568355+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T21:34:53.542+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='notify_success', run_id='manual__2024-10-05T14:29:40.568355+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2024-10-05T21:34:53.543+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'notify_success', 'manual__2024-10-05T14:29:40.568355+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:34:53.550+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'notify_success', 'manual__2024-10-05T14:29:40.568355+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:34:55.561+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:34:55.574+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T21:34:55.670+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:34:56.153+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T21:34:56.154+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:34:56.168+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:34:56.207+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:29:40.568355+00:00 [queued]> on host linux-ip-147
[2024-10-05T21:34:57.217+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='notify_success', run_id='manual__2024-10-05T14:29:40.568355+00:00', try_number=2, map_index=-1)
[2024-10-05T21:34:57.244+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=notify_success, run_id=manual__2024-10-05T14:29:40.568355+00:00, map_index=-1, run_start_date=2024-10-05 14:34:56.297814+00:00, run_end_date=2024-10-05 14:34:56.583365+00:00, run_duration=0.285551, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=48, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-10-05 14:34:53.537709+00:00, queued_by_job_id=1, pid=256780
[2024-10-05T21:34:57.284+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T21:34:57.581+0700] {dagrun.py:823} ERROR - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-05 14:29:40.568355+00:00: manual__2024-10-05T14:29:40.568355+00:00, state:running, queued_at: 2024-10-05 14:29:40.595318+00:00. externally triggered: True> failed
Dag run  in failure state
Dag information:talend_job_dag_Sync_Mysql_to_DWh_job Run id: manual__2024-10-05T14:29:40.568355+00:00 external trigger: True
Failed with message: task_failure
[2024-10-05T21:34:57.582+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-05 14:29:40.568355+00:00, run_id=manual__2024-10-05T14:29:40.568355+00:00, run_start_date=2024-10-05 14:29:41.234322+00:00, run_end_date=2024-10-05 14:34:57.582319+00:00, run_duration=316.347997, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-10-04 08:00:00+00:00, data_interval_end=2024-10-05 08:00:00+00:00, dag_hash=2eab5749fefaddd1b80f7b1e084a79ed
[2024-10-05T21:35:34.771+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:30:18.597436+00:00 [scheduled]>
[2024-10-05T21:35:34.773+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T21:35:34.774+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:30:18.597436+00:00 [scheduled]>
[2024-10-05T21:35:34.778+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:30:18.597436+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T21:35:34.780+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='notify_success', run_id='manual__2024-10-05T14:30:18.597436+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2024-10-05T21:35:34.781+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'notify_success', 'manual__2024-10-05T14:30:18.597436+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:35:34.787+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'notify_success', 'manual__2024-10-05T14:30:18.597436+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:35:36.978+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:35:36.992+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T21:35:37.071+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:35:37.493+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T21:35:37.494+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:35:37.508+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:35:37.539+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:30:18.597436+00:00 [queued]> on host linux-ip-147
[2024-10-05T21:35:38.500+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='notify_success', run_id='manual__2024-10-05T14:30:18.597436+00:00', try_number=2, map_index=-1)
[2024-10-05T21:35:38.521+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=notify_success, run_id=manual__2024-10-05T14:30:18.597436+00:00, map_index=-1, run_start_date=2024-10-05 14:35:37.588186+00:00, run_end_date=2024-10-05 14:35:37.783706+00:00, run_duration=0.19552, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=49, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-10-05 14:35:34.776601+00:00, queued_by_job_id=1, pid=256820
[2024-10-05T21:35:38.831+0700] {dagrun.py:823} ERROR - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-05 14:30:18.597436+00:00: manual__2024-10-05T14:30:18.597436+00:00, state:running, queued_at: 2024-10-05 14:30:18.635227+00:00. externally triggered: True> failed
Dag run  in failure state
Dag information:talend_job_dag_Sync_Mysql_to_DWh_job Run id: manual__2024-10-05T14:30:18.597436+00:00 external trigger: True
Failed with message: task_failure
[2024-10-05T21:35:38.832+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-05 14:30:18.597436+00:00, run_id=manual__2024-10-05T14:30:18.597436+00:00, run_start_date=2024-10-05 14:30:20.361749+00:00, run_end_date=2024-10-05 14:35:38.831853+00:00, run_duration=318.470104, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-10-04 08:00:00+00:00, data_interval_end=2024-10-05 08:00:00+00:00, dag_hash=2eab5749fefaddd1b80f7b1e084a79ed
Dag run  in running state
Dag information Queued at: 2024-10-05 14:36:10.601074+00:00 hash info: 2eab5749fefaddd1b80f7b1e084a79ed
[2024-10-05T21:36:11.521+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:36:10.571453+00:00 [scheduled]>
[2024-10-05T21:36:11.523+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T21:36:11.524+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:36:10.571453+00:00 [scheduled]>
[2024-10-05T21:36:11.528+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:36:10.571453+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T21:36:11.530+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T14:36:10.571453+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T21:36:11.531+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T14:36:10.571453+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:36:11.537+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T14:36:10.571453+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:36:13.528+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:36:13.540+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T21:36:13.612+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:36:14.059+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T21:36:14.060+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:36:14.074+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:36:14.109+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:36:10.571453+00:00 [queued]> on host linux-ip-147
[2024-10-05T21:36:19.656+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T14:36:10.571453+00:00', try_number=1, map_index=-1)
[2024-10-05T21:36:19.685+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T14:36:10.571453+00:00, map_index=-1, run_start_date=2024-10-05 14:36:14.187774+00:00, run_end_date=2024-10-05 14:36:19.002782+00:00, run_duration=4.815008, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=50, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 14:36:11.526236+00:00, queued_by_job_id=1, pid=256853
[2024-10-05T21:36:20.148+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:36:10.571453+00:00 [scheduled]>
[2024-10-05T21:36:20.149+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T21:36:20.150+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:36:10.571453+00:00 [scheduled]>
[2024-10-05T21:36:20.154+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:36:10.571453+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T21:36:20.156+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='notify_success', run_id='manual__2024-10-05T14:36:10.571453+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2024-10-05T21:36:20.157+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'notify_success', 'manual__2024-10-05T14:36:10.571453+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:36:20.163+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'notify_success', 'manual__2024-10-05T14:36:10.571453+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:36:23.220+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:36:23.235+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T21:36:23.319+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:36:23.958+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T21:36:23.960+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:36:24.040+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:36:24.112+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:36:10.571453+00:00 [queued]> on host linux-ip-147
[2024-10-05T21:36:25.216+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='notify_success', run_id='manual__2024-10-05T14:36:10.571453+00:00', try_number=1, map_index=-1)
[2024-10-05T21:36:25.318+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=notify_success, run_id=manual__2024-10-05T14:36:10.571453+00:00, map_index=-1, run_start_date=2024-10-05 14:36:24.176254+00:00, run_end_date=2024-10-05 14:36:24.425763+00:00, run_duration=0.249509, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=51, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-10-05 14:36:20.152329+00:00, queued_by_job_id=1, pid=256880
[2024-10-05T21:39:57.596+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T21:41:25.019+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:36:10.571453+00:00 [scheduled]>
[2024-10-05T21:41:25.021+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T21:41:25.022+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:36:10.571453+00:00 [scheduled]>
[2024-10-05T21:41:25.027+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:36:10.571453+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T21:41:25.028+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='notify_success', run_id='manual__2024-10-05T14:36:10.571453+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2024-10-05T21:41:25.029+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'notify_success', 'manual__2024-10-05T14:36:10.571453+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:41:25.036+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'notify_success', 'manual__2024-10-05T14:36:10.571453+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:41:27.404+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:41:27.418+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T21:41:27.497+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:41:27.971+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T21:41:27.972+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:41:27.986+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:41:28.021+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:36:10.571453+00:00 [queued]> on host linux-ip-147
[2024-10-05T21:41:28.870+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='notify_success', run_id='manual__2024-10-05T14:36:10.571453+00:00', try_number=2, map_index=-1)
[2024-10-05T21:41:28.881+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=notify_success, run_id=manual__2024-10-05T14:36:10.571453+00:00, map_index=-1, run_start_date=2024-10-05 14:41:28.071907+00:00, run_end_date=2024-10-05 14:41:28.299220+00:00, run_duration=0.227313, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=52, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-10-05 14:41:25.024653+00:00, queued_by_job_id=1, pid=257154
[2024-10-05T21:41:29.409+0700] {dagrun.py:823} ERROR - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-05 14:36:10.571453+00:00: manual__2024-10-05T14:36:10.571453+00:00, state:running, queued_at: 2024-10-05 14:36:10.601074+00:00. externally triggered: True> failed
Dag run  in failure state
Dag information:talend_job_dag_Sync_Mysql_to_DWh_job Run id: manual__2024-10-05T14:36:10.571453+00:00 external trigger: True
Failed with message: task_failure
[2024-10-05T21:41:29.410+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-05 14:36:10.571453+00:00, run_id=manual__2024-10-05T14:36:10.571453+00:00, run_start_date=2024-10-05 14:36:11.463924+00:00, run_end_date=2024-10-05 14:41:29.410370+00:00, run_duration=317.946446, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-10-04 08:00:00+00:00, data_interval_end=2024-10-05 08:00:00+00:00, dag_hash=2eab5749fefaddd1b80f7b1e084a79ed
Dag run  in running state
Dag information Queued at: 2024-10-05 14:42:43.925367+00:00 hash info: 2eab5749fefaddd1b80f7b1e084a79ed
[2024-10-05T21:42:45.169+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:42:43.892389+00:00 [scheduled]>
[2024-10-05T21:42:45.178+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T21:42:45.179+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:42:43.892389+00:00 [scheduled]>
[2024-10-05T21:42:45.196+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:42:43.892389+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T21:42:45.197+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T14:42:43.892389+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-05T21:42:45.198+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T14:42:43.892389+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:42:45.209+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'manual__2024-10-05T14:42:43.892389+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:42:46.930+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:42:46.942+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T21:42:47.006+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:42:47.413+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T21:42:47.414+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:42:47.427+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:42:47.458+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job manual__2024-10-05T14:42:43.892389+00:00 [queued]> on host linux-ip-147
[2024-10-05T21:42:52.032+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='manual__2024-10-05T14:42:43.892389+00:00', try_number=1, map_index=-1)
[2024-10-05T21:42:52.044+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=manual__2024-10-05T14:42:43.892389+00:00, map_index=-1, run_start_date=2024-10-05 14:42:47.539032+00:00, run_end_date=2024-10-05 14:42:51.311402+00:00, run_duration=3.77237, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=53, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-05 14:42:45.187446+00:00, queued_by_job_id=1, pid=257227
[2024-10-05T21:42:52.495+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:42:43.892389+00:00 [scheduled]>
[2024-10-05T21:42:52.497+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-05T21:42:52.498+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:42:43.892389+00:00 [scheduled]>
[2024-10-05T21:42:52.503+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:42:43.892389+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-05T21:42:52.505+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='notify_success', run_id='manual__2024-10-05T14:42:43.892389+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2024-10-05T21:42:52.506+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'notify_success', 'manual__2024-10-05T14:42:43.892389+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:42:52.514+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'notify_success', 'manual__2024-10-05T14:42:43.892389+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-05T21:42:54.272+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:42:54.283+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-05T21:42:54.344+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:42:54.736+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-05T21:42:54.737+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-05T21:42:54.750+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-05T21:42:54.779+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success manual__2024-10-05T14:42:43.892389+00:00 [queued]> on host linux-ip-147
[2024-10-05T21:42:56.902+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='notify_success', run_id='manual__2024-10-05T14:42:43.892389+00:00', try_number=1, map_index=-1)
[2024-10-05T21:42:56.913+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=notify_success, run_id=manual__2024-10-05T14:42:43.892389+00:00, map_index=-1, run_start_date=2024-10-05 14:42:54.857139+00:00, run_end_date=2024-10-05 14:42:56.258232+00:00, run_duration=1.401093, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=54, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-10-05 14:42:52.501357+00:00, queued_by_job_id=1, pid=257254
[2024-10-05T21:42:57.226+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-05 14:42:43.892389+00:00: manual__2024-10-05T14:42:43.892389+00:00, state:running, queued_at: 2024-10-05 14:42:43.925367+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2024-10-05 14:42:45.100024+00:00 end:2024-10-05 14:42:57.230186+00:00
[2024-10-05T21:42:57.230+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-05 14:42:43.892389+00:00, run_id=manual__2024-10-05T14:42:43.892389+00:00, run_start_date=2024-10-05 14:42:45.100024+00:00, run_end_date=2024-10-05 14:42:57.230186+00:00, run_duration=12.130162, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-04 08:00:00+00:00, data_interval_end=2024-10-05 08:00:00+00:00, dag_hash=2eab5749fefaddd1b80f7b1e084a79ed
[2024-10-05T21:44:57.981+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T21:49:58.294+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T21:54:58.336+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T21:59:58.756+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T22:04:59.030+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T22:09:59.120+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T22:14:59.320+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T22:19:59.725+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T22:25:00.004+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T22:30:00.307+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T22:35:00.489+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T22:40:00.777+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T22:45:00.977+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T22:50:01.177+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T22:55:01.555+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T23:00:01.940+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T23:05:02.248+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T23:10:02.567+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T23:15:02.979+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T23:20:03.375+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T23:25:03.779+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T23:30:03.979+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T23:35:04.380+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T23:40:04.669+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T23:45:04.987+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T23:50:05.400+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-05T23:55:05.613+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T00:00:06.000+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T00:05:06.416+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T00:10:06.822+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T00:15:07.220+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T00:20:07.639+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T00:25:08.048+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T00:30:08.266+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T00:35:08.452+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T00:40:08.765+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T00:45:08.829+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T00:50:09.050+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T00:55:09.875+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T01:00:10.147+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T01:05:10.219+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T01:10:10.507+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T01:15:10.561+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T01:20:10.855+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T01:25:11.245+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T01:30:11.628+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T01:35:11.740+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T01:40:12.142+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T01:45:12.556+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T01:50:12.841+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T01:55:13.132+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T02:00:13.332+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T02:05:13.649+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T02:10:14.138+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T02:15:14.419+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T02:20:14.623+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T02:25:14.965+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T02:30:15.250+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T02:35:15.857+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T02:40:16.147+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T02:45:16.546+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T02:50:16.829+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T02:55:17.246+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T03:00:17.655+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T03:05:17.739+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T03:10:18.129+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T03:15:18.517+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T03:20:18.926+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T03:25:19.144+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T03:30:19.209+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T03:35:19.623+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T03:40:20.024+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T03:45:20.330+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T03:50:20.750+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T03:55:21.137+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T04:00:21.543+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T04:05:21.831+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T04:10:22.425+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T04:15:22.815+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T04:20:23.257+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T04:25:23.669+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T04:30:24.054+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T04:35:24.457+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T04:40:24.735+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T04:45:25.127+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T04:50:25.502+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T04:55:25.894+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T05:00:26.277+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T05:05:26.675+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T05:10:26.973+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T05:15:27.271+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T05:20:27.658+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T05:25:28.154+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T05:30:28.552+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T05:35:28.963+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T05:40:29.371+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T05:45:29.861+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T05:50:30.057+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T05:55:30.484+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T06:00:30.653+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T06:05:31.070+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T06:10:31.353+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T06:15:31.639+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T06:20:31.706+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T06:25:32.090+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T06:30:32.158+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T06:35:32.440+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T06:40:32.635+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T06:45:33.047+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T06:50:33.448+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T06:55:33.583+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T07:00:33.988+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T07:05:34.391+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T07:10:34.813+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T07:15:35.227+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T07:20:35.750+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T07:25:35.977+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T07:30:36.208+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T07:35:36.615+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T07:40:37.106+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T07:45:37.508+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T07:50:37.918+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T07:55:38.312+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T08:00:38.625+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T08:05:39.030+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T08:10:39.439+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T08:15:39.669+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T08:20:40.060+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T08:25:40.451+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T08:30:40.535+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T08:35:40.940+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T08:40:40.998+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T08:45:41.378+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T08:50:41.476+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T08:55:42.091+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T09:00:42.362+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T09:05:42.671+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T09:10:43.065+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T09:15:43.567+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T09:20:43.964+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T09:25:44.049+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T09:30:44.130+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T09:35:44.243+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T09:40:44.657+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T09:45:45.055+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T09:50:45.255+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T09:55:45.452+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T10:00:45.823+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T10:05:46.240+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T10:10:46.622+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T10:15:46.832+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T10:20:47.227+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T10:25:47.616+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T10:30:48.032+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T10:35:48.258+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T10:40:48.565+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T10:45:48.970+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T10:50:49.355+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T10:55:49.760+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T11:00:50.166+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T11:05:50.599+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T11:10:50.983+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T11:15:51.292+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T11:20:51.577+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T11:25:51.978+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T11:30:52.129+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T11:35:52.416+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T11:40:52.760+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T11:45:53.170+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T11:50:53.617+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T11:55:54.016+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T12:00:54.389+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T12:05:54.766+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T12:10:55.173+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T12:15:55.198+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T12:20:55.618+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T12:25:56.019+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T12:30:56.135+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T12:35:56.456+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T12:40:56.858+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T12:45:57.266+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T12:50:57.669+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T12:55:57.882+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T13:00:58.081+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T13:05:58.244+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T13:10:58.257+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T13:15:58.547+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T13:20:58.747+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T13:25:59.264+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T13:30:59.557+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T13:36:00.069+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T13:41:00.456+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T13:46:00.656+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T13:51:01.070+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T13:56:01.380+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T14:01:01.767+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T14:06:02.152+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T14:11:02.509+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T14:16:02.898+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T14:21:03.301+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T14:26:03.645+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T14:31:03.882+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T14:36:04.302+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T14:41:04.727+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T14:46:05.059+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T14:51:05.268+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T14:56:05.656+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T15:00:01.155+0700] {dag.py:4180} INFO - Setting next_dagrun for talend_job_dag_Sync_Mysql_to_DWh_job to 2024-10-06 08:00:00+00:00, run_after=2024-10-07 08:00:00+00:00
Dag run  in running state
Dag information Queued at: 2024-10-06 08:00:01.134374+00:00 hash info: 2eab5749fefaddd1b80f7b1e084a79ed
[2024-10-06T15:00:01.238+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job scheduled__2024-10-05T08:00:00+00:00 [scheduled]>
[2024-10-06T15:00:01.239+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-06T15:00:01.240+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job scheduled__2024-10-05T08:00:00+00:00 [scheduled]>
[2024-10-06T15:00:01.244+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job scheduled__2024-10-05T08:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-06T15:00:01.245+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='scheduled__2024-10-05T08:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-10-06T15:00:01.246+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'scheduled__2024-10-05T08:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-06T15:00:01.252+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'run_talend_job', 'scheduled__2024-10-05T08:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-06T15:00:03.702+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-06T15:00:03.731+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-06T15:00:03.870+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-06T15:00:04.500+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-06T15:00:04.500+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-06T15:00:04.517+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-06T15:00:04.558+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.run_talend_job scheduled__2024-10-05T08:00:00+00:00 [queued]> on host linux-ip-147
[2024-10-06T15:00:09.217+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='run_talend_job', run_id='scheduled__2024-10-05T08:00:00+00:00', try_number=1, map_index=-1)
[2024-10-06T15:00:09.242+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=run_talend_job, run_id=scheduled__2024-10-05T08:00:00+00:00, map_index=-1, run_start_date=2024-10-06 08:00:04.628408+00:00, run_end_date=2024-10-06 08:00:08.400810+00:00, run_duration=3.772402, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=55, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-10-06 08:00:01.242095+00:00, queued_by_job_id=1, pid=315973
[2024-10-06T15:00:09.725+0700] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success scheduled__2024-10-05T08:00:00+00:00 [scheduled]>
[2024-10-06T15:00:09.727+0700] {scheduler_job_runner.py:495} INFO - DAG talend_job_dag_Sync_Mysql_to_DWh_job has 0/16 running and queued tasks
[2024-10-06T15:00:09.728+0700] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success scheduled__2024-10-05T08:00:00+00:00 [scheduled]>
[2024-10-06T15:00:09.732+0700] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success scheduled__2024-10-05T08:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-10-06T15:00:09.733+0700] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='notify_success', run_id='scheduled__2024-10-05T08:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2024-10-06T15:00:09.734+0700] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'notify_success', 'scheduled__2024-10-05T08:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-06T15:00:09.741+0700] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'talend_job_dag_Sync_Mysql_to_DWh_job', 'notify_success', 'scheduled__2024-10-05T08:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/Sync_Mysql_to_DWh_job.py']
[2024-10-06T15:00:11.585+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-06T15:00:11.598+0700] {dagbag.py:588} INFO - Filling up the DagBag from /root/airflow/dags/Sync_Mysql_to_DWh_job.py
[2024-10-06T15:00:11.663+0700] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-06T15:00:12.149+0700] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-06T15:00:12.150+0700] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-06T15:00:12.163+0700] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2024-10-06T15:00:12.199+0700] {task_command.py:467} INFO - Running <TaskInstance: talend_job_dag_Sync_Mysql_to_DWh_job.notify_success scheduled__2024-10-05T08:00:00+00:00 [queued]> on host linux-ip-147
[2024-10-06T15:00:14.600+0700] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='talend_job_dag_Sync_Mysql_to_DWh_job', task_id='notify_success', run_id='scheduled__2024-10-05T08:00:00+00:00', try_number=1, map_index=-1)
[2024-10-06T15:00:14.625+0700] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, task_id=notify_success, run_id=scheduled__2024-10-05T08:00:00+00:00, map_index=-1, run_start_date=2024-10-06 08:00:12.289204+00:00, run_end_date=2024-10-06 08:00:13.669518+00:00, run_duration=1.380314, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=56, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-10-06 08:00:09.729721+00:00, queued_by_job_id=1, pid=315998
[2024-10-06T15:00:14.840+0700] {dagrun.py:854} INFO - Marking run <DagRun talend_job_dag_Sync_Mysql_to_DWh_job @ 2024-10-05 08:00:00+00:00: scheduled__2024-10-05T08:00:00+00:00, state:running, queued_at: 2024-10-06 08:00:01.134374+00:00. externally triggered: False> successful
Dag run in success state
Dag run start:2024-10-06 08:00:01.178637+00:00 end:2024-10-06 08:00:14.841702+00:00
[2024-10-06T15:00:14.842+0700] {dagrun.py:905} INFO - DagRun Finished: dag_id=talend_job_dag_Sync_Mysql_to_DWh_job, execution_date=2024-10-05 08:00:00+00:00, run_id=scheduled__2024-10-05T08:00:00+00:00, run_start_date=2024-10-06 08:00:01.178637+00:00, run_end_date=2024-10-06 08:00:14.841702+00:00, run_duration=13.663065, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-10-05 08:00:00+00:00, data_interval_end=2024-10-06 08:00:00+00:00, dag_hash=2eab5749fefaddd1b80f7b1e084a79ed
[2024-10-06T15:00:14.856+0700] {dag.py:4180} INFO - Setting next_dagrun for talend_job_dag_Sync_Mysql_to_DWh_job to 2024-10-06 08:00:00+00:00, run_after=2024-10-07 08:00:00+00:00
[2024-10-06T15:01:05.949+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T15:06:06.359+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T15:11:06.748+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T15:16:07.174+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T15:21:07.572+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T15:26:07.983+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T15:31:08.382+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T15:36:08.755+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T15:41:08.959+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T15:46:09.002+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T15:51:09.397+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T15:56:09.422+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T16:01:09.836+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T16:06:10.242+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T16:11:10.468+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T16:16:10.972+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T16:21:11.172+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T16:26:11.366+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T16:31:11.418+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T16:36:11.919+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T16:41:12.420+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T16:46:12.835+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T16:51:13.133+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T16:56:13.513+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T17:01:13.566+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T17:06:13.963+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T17:11:14.355+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T17:16:14.753+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T17:21:15.161+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T17:26:15.362+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T17:31:15.770+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T17:36:16.063+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T17:41:16.455+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T17:46:16.838+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T17:51:17.084+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T17:56:17.482+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T18:01:17.881+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T18:06:17.979+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T18:11:18.366+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T18:16:18.784+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T18:21:18.982+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T18:26:19.462+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T18:31:19.844+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T18:36:20.018+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T18:41:20.074+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T18:46:20.486+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T18:51:20.890+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T18:56:21.322+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T19:01:21.527+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T19:06:21.701+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T19:11:21.714+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T19:16:22.128+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T19:21:22.165+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T19:26:22.568+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T19:31:22.748+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T19:36:22.795+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T19:41:23.395+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T19:46:23.797+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T19:51:24.096+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T19:56:24.192+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T20:01:24.361+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T20:06:24.733+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T20:11:25.139+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T20:16:25.555+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T20:21:25.945+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T20:26:26.139+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T20:31:26.530+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T20:36:26.683+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T20:41:26.875+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T20:46:27.122+0700] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-06T20:49:40.839+0700] {job.py:239} ERROR - Job heartbeat failed with error
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: database or disk is full

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/airflow/jobs/job.py", line 224, in heartbeat
    job = Job._update_heartbeat(job=self, session=session)
  File "/usr/local/lib/python3.10/dist-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/airflow/utils/retries.py", line 93, in wrapped_function
    for attempt in run_with_db_retries(max_retries=retries, logger=logger, **retry_kwargs):
  File "/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
  File "/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/usr/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.10/dist-packages/airflow/utils/retries.py", line 102, in wrapped_function
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/airflow/jobs/job.py", line 379, in _update_heartbeat
    session.commit()
  File "/usr/local/lib/python3.10/dist-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/usr/local/lib/python3.10/dist-packages/sqlalchemy/orm/session.py", line 832, in commit
    self._prepare_impl()
  File "/usr/local/lib/python3.10/dist-packages/sqlalchemy/orm/session.py", line 811, in _prepare_impl
    self.session.flush()
  File "/usr/local/lib/python3.10/dist-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/usr/local/lib/python3.10/dist-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/usr/local/lib/python3.10/dist-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/usr/local/lib/python3.10/dist-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/usr/local/lib/python3.10/dist-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/usr/local/lib/python3.10/dist-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/usr/local/lib/python3.10/dist-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/usr/local/lib/python3.10/dist-packages/sqlalchemy/orm/persistence.py", line 237, in save_obj
    _emit_update_statements(
  File "/usr/local/lib/python3.10/dist-packages/sql