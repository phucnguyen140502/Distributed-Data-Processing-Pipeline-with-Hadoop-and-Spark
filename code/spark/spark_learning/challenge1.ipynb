{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdd40045-95ae-4b83-aefe-954a785094af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CV compare\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"7g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.network.timeout\", \"30000s\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"64m\")  \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\")  \\\n",
    "    .config(\"spark.default.parallelism\", \"16\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"20000s\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4560eb99-b371-4e12-99ab-d284f85f0e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Thời gian chạy với HDFS: 724.73 giây\n"
     ]
    }
   ],
   "source": [
    "start_time_hdfs_rdd = time.time()\n",
    "citizens_rdd_hdfs = spark.sparkContext.textFile(\"hdfs://172.30.2.147:9000/demo_directory/citizens_data.csv\")\n",
    "citizens_rdd_hdfs = citizens_rdd_hdfs.repartition(8)\n",
    "header = citizens_rdd_hdfs.first()\n",
    "data_citizens_rdd_hdfs = citizens_rdd_hdfs.filter(lambda line: line != header)\n",
    "\n",
    "print(data_citizens_rdd_hdfs.filter(lambda x: x.split(\",\")[4] == 'French').collect())\n",
    "\n",
    "end_time_hdfs_rdd = time.time()\n",
    "time_hdfs_rdd = end_time_hdfs_rdd - start_time_hdfs_rdd\n",
    "print(\"Thời gian chạy với HDFS: {:.2f} giây\".format(time_hdfs_rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b479cbe3-3bc0-4ec1-a63c-902896b85a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lần từ 'french' xuất hiện trong toàn bộ các cột: 1155146\n",
      "Thời gian chạy với HDFS: 380.26 giây\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_extract\n",
    "import time\n",
    "\n",
    "# Khởi tạo SparkSession với các cấu hình tối ưu\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CV compare\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.network.timeout\", \"30000s\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"512\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128m\") \\\n",
    "    .config(\"spark.rdd.compress\", \"true\") \\\n",
    "    .config(\"spark.io.compression.codec\", \"snappy\") \\\n",
    "    .config(\"spark.shuffle.file.buffer\", \"128k\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\")  \\\n",
    "    .config(\"spark.default.parallelism\", \"16\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"20000s\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=25 -XX:MaxGCPauseMillis=100\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "start_time_hdfs_rdd = time.time()\n",
    "\n",
    "# Đường dẫn tới file Parquet\n",
    "parquet_path = r\"D:\\Data Engineer\\dwh and etl\\lesson\\lesson 2\\documents\\Talend\\Sync Hadoop to DWH\\citizens_data.parquet\"\n",
    "\n",
    "# Kiểm tra nếu tệp Parquet đã tồn tại\n",
    "if not os.path.exists(parquet_path):\n",
    "    # Đọc file CSV và chuyển thành DataFrame\n",
    "    df = spark.read.csv(r\"D:\\Data Engineer\\dwh and etl\\lesson\\lesson 2\\documents\\Talend\\Sync Hadoop to DWH\\citizens_data.csv\", \n",
    "                        header=True, inferSchema=True)\n",
    "\n",
    "    df.cache()\n",
    "\n",
    "    # Ghi dữ liệu dưới dạng Parquet với nén snappy và partitioning theo country\n",
    "    df.write.option(\"compression\", \"snappy\") \\\n",
    "             .partitionBy(\"country\") \\\n",
    "             .mode(\"overwrite\") \\\n",
    "             .parquet(parquet_path)\n",
    "\n",
    "# Đọc dữ liệu Parquet\n",
    "df_parquet = spark.read.parquet(parquet_path)\n",
    "\n",
    "# Giảm số partition nếu cần thiết\n",
    "df_parquet = df_parquet.coalesce(16)\n",
    "\n",
    "# Khởi tạo biến đếm tổng số lần xuất hiện của từ 'french'\n",
    "total_count_french = 0\n",
    "\n",
    "# Duyệt qua tất cả các cột\n",
    "for column in df_parquet.columns:\n",
    "    # Lọc các từ chứa \"french\" không phân biệt hoa/thường và đếm số lần xuất hiện\n",
    "    count_french_in_column = df_parquet \\\n",
    "        .withColumn(\"french_count\", \n",
    "                    regexp_extract(lower(col(column)), \"(french)\", 0)) \\\n",
    "        .filter(col(\"french_count\") != \"\") \\\n",
    "        .count()\n",
    "    \n",
    "    total_count_french += count_french_in_column\n",
    "\n",
    "# In kết quả\n",
    "print(f\"Số lần từ 'french' xuất hiện trong toàn bộ các cột: {total_count_french}\")\n",
    "\n",
    "# Thời gian thực thi\n",
    "end_time_hdfs_rdd = time.time()\n",
    "time_hdfs_rdd = end_time_hdfs_rdd - start_time_hdfs_rdd\n",
    "print(\"Thời gian chạy với HDFS: {:.2f} giây\".format(time_hdfs_rdd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76055f91-bba5-4164-ad8a-4c4acf0b1b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lần từ 'french' xuất hiện trong cột name, country: 1155146\n",
      "Thời gian chạy với HDFS: 26.19 giây\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_extract\n",
    "import time\n",
    "\n",
    "# Khởi tạo SparkSession với các cấu hình tối ưu\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CV compare\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.instances\", \"8\") \\\n",
    "    .config(\"spark.network.timeout\", \"30000s\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"512\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128m\") \\\n",
    "    .config(\"spark.rdd.compress\", \"true\") \\\n",
    "    .config(\"spark.io.compression.codec\", \"snappy\") \\\n",
    "    .config(\"spark.shuffle.file.buffer\", \"128k\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\")  \\\n",
    "    .config(\"spark.default.parallelism\", \"16\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"20000s\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=25 -XX:MaxGCPauseMillis=100\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "start_time_hdfs_rdd = time.time()\n",
    "\n",
    "# Đường dẫn tới file Parquet\n",
    "parquet_path = r\"D:\\Data Engineer\\dwh and etl\\lesson\\lesson 2\\documents\\Talend\\Sync Hadoop to DWH\\citizens_data.parquet\"\n",
    "\n",
    "# Kiểm tra nếu tệp Parquet đã tồn tại\n",
    "if not os.path.exists(parquet_path):\n",
    "    # Đọc file CSV và chuyển thành DataFrame\n",
    "    df = spark.read.csv(r\"D:\\Data Engineer\\dwh and etl\\lesson\\lesson 2\\documents\\Talend\\Sync Hadoop to DWH\\citizens_data.csv\", \n",
    "                        header=True, inferSchema=True)\n",
    "\n",
    "    df.cache()\n",
    "\n",
    "    # Ghi dữ liệu dưới dạng Parquet với nén snappy và partitioning theo country\n",
    "    df.write.option(\"compression\", \"snappy\") \\\n",
    "             .partitionBy(\"country\") \\\n",
    "             .mode(\"overwrite\") \\\n",
    "             .parquet(parquet_path)\n",
    "\n",
    "# Đọc dữ liệu Parquet\n",
    "df_parquet = spark.read.parquet(parquet_path)\n",
    "\n",
    "# Giảm số partition nếu cần thiết\n",
    "df_parquet = df_parquet.repartition(16)\n",
    "\n",
    "# Chỉ kiểm tra trên 2 cột name và country\n",
    "columns_to_check = [\"name\", \"country\"]\n",
    "\n",
    "# Khởi tạo biến đếm tổng số lần xuất hiện của từ 'french'\n",
    "total_count_french = 0\n",
    "\n",
    "# Tính tổng số lần xuất hiện của từ 'french' trong các cột được chọn\n",
    "for column in columns_to_check:\n",
    "    french_count_in_column = df_parquet \\\n",
    "        .withColumn(\"french_count\", \n",
    "                    regexp_extract(lower(col(column)), \"(french)\", 0)) \\\n",
    "        .filter(col(\"french_count\") != \"\") \\\n",
    "        .count()\n",
    "    \n",
    "    total_count_french += french_count_in_column\n",
    "\n",
    "# In kết quả\n",
    "print(f\"Số lần từ 'french' xuất hiện trong cột {', '.join(columns_to_check)}: {total_count_french}\")\n",
    "\n",
    "# Thời gian thực thi\n",
    "end_time_hdfs_rdd = time.time()\n",
    "time_hdfs_rdd = end_time_hdfs_rdd - start_time_hdfs_rdd\n",
    "print(\"Thời gian chạy với HDFS: {:.2f} giây\".format(time_hdfs_rdd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1c9be5d-150d-4a32-b8cc-976a6944a3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask[distributed] in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2024.11.0)\n",
      "Collecting dask[distributed]\n",
      "  Downloading dask-2024.11.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: click>=8.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dask[distributed]) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dask[distributed]) (3.1.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dask[distributed]) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dask[distributed]) (24.1)\n",
      "Requirement already satisfied: partd>=1.4.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dask[distributed]) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dask[distributed]) (6.0.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dask[distributed]) (1.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dask[distributed]) (8.5.0)\n",
      "Collecting distributed==2024.11.2 (from dask[distributed])\n",
      "  Downloading distributed-2024.11.2-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from distributed==2024.11.2->dask[distributed]) (3.1.4)\n",
      "Requirement already satisfied: locket>=1.0.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from distributed==2024.11.2->dask[distributed]) (1.0.0)\n",
      "Collecting msgpack>=1.0.2 (from distributed==2024.11.2->dask[distributed])\n",
      "  Downloading msgpack-1.1.0-cp310-cp310-win_amd64.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from distributed==2024.11.2->dask[distributed]) (6.1.0)\n",
      "Collecting sortedcontainers>=2.0.5 (from distributed==2024.11.2->dask[distributed])\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting tblib>=1.6.0 (from distributed==2024.11.2->dask[distributed])\n",
      "  Using cached tblib-3.0.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: tornado>=6.2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from distributed==2024.11.2->dask[distributed]) (6.4.1)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from distributed==2024.11.2->dask[distributed]) (2.2.3)\n",
      "Collecting zict>=3.0.0 (from distributed==2024.11.2->dask[distributed])\n",
      "  Using cached zict-3.0.0-py2.py3-none-any.whl.metadata (899 bytes)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click>=8.1->dask[distributed]) (0.4.6)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from importlib-metadata>=4.13.0->dask[distributed]) (3.20.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2>=2.10.3->distributed==2024.11.2->dask[distributed]) (3.0.2)\n",
      "Downloading distributed-2024.11.2-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.5/1.0 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 1.8 MB/s eta 0:00:00\n",
      "Downloading dask-2024.11.2-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.3/1.3 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.5/1.3 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.0/1.3 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 1.7 MB/s eta 0:00:00\n",
      "Downloading msgpack-1.1.0-cp310-cp310-win_amd64.whl (74 kB)\n",
      "Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Using cached tblib-3.0.0-py3-none-any.whl (12 kB)\n",
      "Using cached zict-3.0.0-py2.py3-none-any.whl (43 kB)\n",
      "Installing collected packages: sortedcontainers, zict, tblib, msgpack, dask, distributed\n",
      "  Attempting uninstall: dask\n",
      "    Found existing installation: dask 2024.11.0\n",
      "    Uninstalling dask-2024.11.0:\n",
      "      Successfully uninstalled dask-2024.11.0\n",
      "Successfully installed dask-2024.11.2 distributed-2024.11.2 msgpack-1.1.0 sortedcontainers-2.4.0 tblib-3.0.0 zict-3.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"dask[distributed]\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25c8a6de-205a-4b63-89c3-bb313746ba9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs: 8\n",
      "Number of Workers: 7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Xác định số CPU có trên hệ thống\n",
    "num_cpus = os.cpu_count()\n",
    "\n",
    "# Điều chỉnh số worker (mặc định là số CPU)\n",
    "num_workers = max(1, num_cpus - 1)  # Trừ đi 1 để dành tài nguyên cho hệ thống\n",
    "print(f\"Number of CPUs: {num_cpus}\")\n",
    "print(f\"Number of Workers: {num_workers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0cb336-2e0c-4656-88f5-8c7fb5d9d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import threading\n",
    "import psutil\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "import gc\n",
    "\n",
    "# Cấu hình logging\n",
    "logging.basicConfig(\n",
    "    filename=r\"D:\\Data Engineer\\dwh and etl\\lesson\\lesson 2\\documents\\Hadoop\\log_read_citizens.log\",\n",
    "    level=logging.INFO,\n",
    "    format='[%(asctime)s] [%(levelname)s] %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    force=True\n",
    ")\n",
    "\n",
    "def wait_for_memory_to_cool(threshold=95, cooldown=80, check_interval=10):\n",
    "    \"\"\"Chờ cho đến khi RAM hạ nhiệt xuống dưới mức cooldown.\"\"\"\n",
    "    while psutil.virtual_memory().percent >= threshold:\n",
    "        logging.warning(f\"Bộ nhớ RAM vẫn cao ({psutil.virtual_memory().percent}%). Chờ {check_interval} giây...\")\n",
    "        time.sleep(check_interval)\n",
    "    logging.info(f\"Bộ nhớ RAM đã hạ nhiệt ({psutil.virtual_memory().percent}%). Tiếp tục xử lý.\")\n",
    "\n",
    "def save_chunk_to_temp(chunk, temp_file_path, thread_id, chunk_id):\n",
    "    \"\"\"Lưu chunk đã xử lý tạm thời vào file CSV.\"\"\"\n",
    "    temp_dir = os.path.join(\"tmp\")\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    temp_chunk_file = os.path.join(temp_dir, f\"{temp_file_path}_thread_{thread_id}_chunk_{chunk_id}.csv\")\n",
    "    chunk.to_csv(temp_chunk_file, index=False)\n",
    "    logging.info(f\"Chunk {chunk_id} của thread {thread_id} đã được lưu vào file tạm: {temp_chunk_file}\")\n",
    "    return temp_chunk_file\n",
    "\n",
    "def count_french_words(chunk):\n",
    "    \"\"\"Đếm số lần từ 'french' xuất hiện trong các cột 'name' và 'country'.\"\"\"\n",
    "    french_word_count = 0\n",
    "    french_word = \"french\"\n",
    "    \n",
    "    # Kiểm tra trong cột 'name'\n",
    "    if 'name' in chunk.columns:\n",
    "        french_word_count += chunk['name'].apply(lambda x: str(x).lower().count(french_word)).sum()\n",
    "    \n",
    "    # Kiểm tra trong cột 'country'\n",
    "    if 'country' in chunk.columns:\n",
    "        french_word_count += chunk['country'].apply(lambda x: str(x).lower().count(french_word)).sum()\n",
    "    \n",
    "    return french_word_count\n",
    "\n",
    "def process_chunk(chunk, thread_id, chunk_id, temp_file_path, processed_chunks):\n",
    "    current_memory = psutil.virtual_memory().percent\n",
    "    \n",
    "    if current_memory >= 97:\n",
    "        logging.warning(f\"Memory usage đạt ngưỡng 97%. Lưu tạm chunk {chunk_id}.\")\n",
    "        temp_file = save_chunk_to_temp(chunk, temp_file_path, thread_id, chunk_id)\n",
    "        return None\n",
    "\n",
    "    processed_chunks.add((thread_id, chunk_id))\n",
    "    processing_time = 0.1\n",
    "    gc.collect()\n",
    "\n",
    "    french_word_count = count_french_words(chunk)\n",
    "    logging.info(f\"Thread {threading.current_thread().name} đã xử lý chunk {chunk_id} và tìm thấy {french_word_count} từ 'french' trong {processing_time:.2f} giây.\")\n",
    "    \n",
    "    return thread_id, chunk_id, chunk, processing_time, french_word_count\n",
    "\n",
    "def read_and_process_csv(file_path, chunk_size, num_threads, temp_file_path):\n",
    "    results = []\n",
    "    total_start_time = time.time()\n",
    "    processed_chunks = set()\n",
    "    total_french_word_count = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = []\n",
    "        chunk_id = 0\n",
    "\n",
    "        for chunk in pd.read_csv(file_path, usecols=['name', 'country'], chunksize=chunk_size):\n",
    "            future = executor.submit(process_chunk, chunk, threading.get_ident(), chunk_id, temp_file_path, processed_chunks)\n",
    "            futures.append(future)\n",
    "            chunk_id += 1\n",
    "\n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                thread_id, chunk_id, processed_chunk, processing_time, french_word_count = result\n",
    "                results.append((thread_id, chunk_id, processed_chunk, processing_time))\n",
    "                total_french_word_count += french_word_count\n",
    "            else:\n",
    "                logging.warning(f\"Một chunk đã tạm dừng xử lý do giới hạn bộ nhớ.\")\n",
    "\n",
    "    total_processing_time = time.time() - total_start_time\n",
    "    # Kết hợp các chunk lại\n",
    "    full_data = pd.concat([result[2] for result in sorted(results, key=lambda x: (x[0], x[1]))], ignore_index=True)\n",
    "    \n",
    "\n",
    "    logging.info(f\"Tổng thời gian xử lý: {total_processing_time:.2f} giây\")\n",
    "    logging.info(f\"Tổng số dòng đã đọc và xử lý: {len(full_data)}\")\n",
    "    logging.info(f\"Tổng số từ 'french' đã tìm thấy: {total_french_word_count}\")\n",
    "    return full_data\n",
    "\n",
    "# File CSV cần đọc\n",
    "file_path = r\"D:\\Data Engineer\\dwh and etl\\lesson\\lesson 2\\documents\\Talend\\Sync Hadoop to DWH\\citizens_data.csv\"\n",
    "chunk_size = 11250000\n",
    "num_threads = 8\n",
    "temp_file_path = \"temp_citizens_data\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        full_data = read_and_process_csv(file_path, chunk_size, num_threads, temp_file_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Lỗi trong quá trình xử lý: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5a5cf77-194d-4960-93d1-8e632be9e893",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Thread-0] Bắt đầu xử lý chunk với 22500000 dòng...\n",
      "[Thread-0] Đã hoàn thành xử lý trong 21.43s.\n",
      "[Thread-0] CPU: 100.3% | RAM: 32.76767512146712% | Số từ 'french': 289490\n",
      "[Thread-1] Bắt đầu xử lý chunk với 22500000 dòng...\n",
      "[Thread-1] Đã hoàn thành xử lý trong 22.63s.\n",
      "[Thread-1] CPU: 100.0% | RAM: 60.95120021691164% | Số từ 'french': 287879\n",
      "[Thread-2] Bắt đầu xử lý chunk với 22500000 dòng...\n",
      "[Thread-2] Đã hoàn thành xử lý trong 22.74s.\n",
      "[Thread-2] CPU: 100.0% | RAM: 40.934407996627975% | Số từ 'french': 289141\n",
      "[Thread-3] Bắt đầu xử lý chunk với 22500000 dòng...\n",
      "[Thread-3] Đã hoàn thành xử lý trong 23.22s.\n",
      "[Thread-3] CPU: 0.0% | RAM: 67.1897380229319% | Số từ 'french': 288636\n",
      "Tổng số dòng: 90000000, tổng số từ 'french': 1155146\n",
      "Tổng thời gian xử lý: 135.58 giây\n",
      "Tổng thời gian thực tế của các chunks: 90.02 giây\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pandas as pd\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def count_french_words(chunk):\n",
    "    \"\"\"Hàm đếm từ 'french' trong chunk\"\"\"\n",
    "    try:\n",
    "        french_word = \"french\"\n",
    "        return (\n",
    "            chunk['name'].str.lower().str.count(french_word).sum() +\n",
    "            chunk['country'].str.lower().str.count(french_word).sum()\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error in count_french_words: {e}\")\n",
    "        return 0\n",
    "\n",
    "def process_chunk(chunk, chunk_index):\n",
    "    \"\"\"Xử lý từng chunk và thông báo chi tiết\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        print(f\"[Thread-{chunk_index}] Bắt đầu xử lý chunk với {len(chunk)} dòng...\")\n",
    "        \n",
    "        # Đếm từ 'french' trong chunk\n",
    "        count = count_french_words(chunk)\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Lấy thông tin tài nguyên (CPU, RAM) của thread đang xử lý\n",
    "        cpu_percent, ram_percent = get_process_resources(os.getpid())\n",
    "        print(f\"[Thread-{chunk_index}] Đã hoàn thành xử lý trong {processing_time:.2f}s.\")\n",
    "        print(f\"[Thread-{chunk_index}] CPU: {cpu_percent}% | RAM: {ram_percent}% | Số từ 'french': {count}\")\n",
    "        \n",
    "        return len(chunk), count, processing_time\n",
    "    except Exception as e:\n",
    "        print(f\"[Thread-{chunk_index}] Lỗi: {e}\")\n",
    "        return 0, 0, 0\n",
    "\n",
    "def get_process_resources(pid=None):\n",
    "    \"\"\"Lấy thông tin tài nguyên của process (CPU, RAM)\"\"\"\n",
    "    try:\n",
    "        process = psutil.Process(pid)\n",
    "        cpu_percent = process.cpu_percent(interval=0.1)  # % CPU\n",
    "        memory_info = process.memory_info()  # RAM info (in bytes)\n",
    "        memory_percent = (memory_info.rss / psutil.virtual_memory().total) * 100  # % RAM\n",
    "        return cpu_percent, memory_percent\n",
    "    except psutil.NoSuchProcess:\n",
    "        return 0, 0\n",
    "\n",
    "def read_and_process_csv(file_path, chunk_size, num_workers):\n",
    "    total_count = 0\n",
    "    total_rows = 0\n",
    "    total_processing_time = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:  # Sử dụng ThreadPoolExecutor\n",
    "        futures = {}\n",
    "        for chunk_index, chunk in enumerate(\n",
    "            pd.read_csv(file_path, chunksize=chunk_size, usecols=['name', 'country'])\n",
    "        ):\n",
    "            try:\n",
    "                # Gửi chunk tới executor\n",
    "                future = executor.submit(process_chunk, chunk, chunk_index)\n",
    "                futures[future] = chunk_index\n",
    "            except Exception as e:\n",
    "                print(f\"Error while submitting chunk {chunk_index}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Đợi kết quả từ tất cả các thread\n",
    "        for future in as_completed(futures):\n",
    "            chunk_index = futures[future]\n",
    "            try:\n",
    "                rows, count, processing_time = future.result()\n",
    "                total_rows += rows\n",
    "                total_count += count\n",
    "                total_processing_time += processing_time\n",
    "            except Exception as e:\n",
    "                print(f\"[Thread-{chunk_index}] Xử lý thất bại với lỗi: {e}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Tổng số dòng: {total_rows}, tổng số từ 'french': {total_count}\")\n",
    "    print(f\"Tổng thời gian xử lý: {end_time - start_time:.2f} giây\")\n",
    "    print(f\"Tổng thời gian thực tế của các chunks: {total_processing_time:.2f} giây\")\n",
    "\n",
    "# Thực thi\n",
    "file_path = r\"D:\\Data Engineer\\dwh and etl\\lesson\\lesson 2\\documents\\Talend\\Sync Hadoop to DWH\\citizens_data.csv\"\n",
    "chunk_size = 22_500_000  # Tăng kích thước chunk citizens_data.parquet\n",
    "num_workers = 4         # Sử dụng 6 luồng (threads) để chạy song song\n",
    "\n",
    "read_and_process_csv(file_path, chunk_size, num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab4f92e-af34-42b5-a93d-ad30ca71c3b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Parquet D:\\Data Engineer\\dwh and etl\\lesson\\lesson 2\\documents\\Talend\\Sync Hadoop to DWH\\citizens_data.parquet đã tồn tại, sẽ đọc từ đó.\n",
      "[Thread-0] Thời gian xử lý: 8.91s, CPU: 100.2% | RAM: 31.079261510940892% | Số từ 'french': 13517\n",
      "[Thread-10] Thời gian xử lý: 16.68s, CPU: 101.4% | RAM: 19.739856397898983% | Số từ 'french': 13487\n",
      "[Thread-20] Thời gian xử lý: 16.33s, CPU: 160.3% | RAM: 27.929192319426637% | Số từ 'french': 13577\n",
      "[Thread-30] Thời gian xử lý: 16.74s, CPU: 99.9% | RAM: 32.996155056898075% | Số từ 'french': 13471\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pandas as pd\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "def count_french_words(chunk):\n",
    "    \"\"\"Đếm từ 'french' trong chunk\"\"\"\n",
    "    try:\n",
    "        french_word = \"french\"\n",
    "        return (\n",
    "            chunk['name'].str.lower().str.count(french_word).sum() +\n",
    "            chunk['country'].str.lower().str.count(french_word).sum()\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error in count_french_words: {e}\")\n",
    "        return 0\n",
    "\n",
    "def process_chunk(chunk, chunk_index):\n",
    "    \"\"\"Xử lý chunk và trả kết quả\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Đếm từ 'french' trong chunk\n",
    "        count = count_french_words(chunk)\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Tránh log không cần thiết nếu không cần\n",
    "        if chunk_index % 10 == 0:  # Chỉ in log mỗi 10 chunk để giảm tải\n",
    "            cpu_percent, ram_percent = get_process_resources(os.getpid())\n",
    "            print(f\"[Thread-{chunk_index}] Thời gian xử lý: {processing_time:.2f}s, CPU: {cpu_percent}% | RAM: {ram_percent}% | Số từ 'french': {count}\")\n",
    "        \n",
    "        return len(chunk), count, processing_time\n",
    "    except Exception as e:\n",
    "        print(f\"[Thread-{chunk_index}] Lỗi: {e}\")\n",
    "        return 0, 0, 0\n",
    "\n",
    "def get_process_resources(pid=None):\n",
    "    \"\"\"Lấy tài nguyên của process (CPU, RAM)\"\"\"\n",
    "    try:\n",
    "        process = psutil.Process(pid)\n",
    "        cpu_percent = process.cpu_percent(interval=0.1)\n",
    "        memory_info = process.memory_info()\n",
    "        memory_percent = (memory_info.rss / psutil.virtual_memory().total) * 100\n",
    "        return cpu_percent, memory_percent\n",
    "    except psutil.NoSuchProcess:\n",
    "        return 0, 0\n",
    "\n",
    "def create_parquet_from_csv(csv_file, parquet_file):\n",
    "    \"\"\"Tạo file Parquet từ CSV nếu chưa có\"\"\"\n",
    "    if not os.path.exists(parquet_file):\n",
    "        print(f\"File {parquet_file} chưa tồn tại, đang chuyển đổi...\")\n",
    "        df = pd.read_csv(csv_file, usecols=['name', 'country'])\n",
    "        table = pa.Table.from_pandas(df)\n",
    "        pq.write_table(table, parquet_file)\n",
    "        print(f\"File Parquet {parquet_file} đã được tạo.\")\n",
    "    else:\n",
    "        print(f\"File Parquet {parquet_file} đã tồn tại, sẽ đọc từ đó.\")\n",
    "\n",
    "def read_and_process_parquet(file_path, chunk_size, num_workers):\n",
    "    total_count = 0\n",
    "    total_rows = 0\n",
    "    total_processing_time = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Đọc dữ liệu từ file Parquet\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = []\n",
    "        \n",
    "        # Đọc dữ liệu theo từng row group (không tải toàn bộ vào RAM)\n",
    "        for chunk_index in range(parquet_file.num_row_groups):\n",
    "            try:\n",
    "                # Đọc 1 row group và chuyển đổi thành pandas DataFrame\n",
    "                chunk = parquet_file.read_row_group(chunk_index).to_pandas()[['name', 'country']]\n",
    "                \n",
    "                # Gửi chunk vào xử lý song song\n",
    "                future = executor.submit(process_chunk, chunk, chunk_index)\n",
    "                futures.append(future)\n",
    "            except Exception as e:\n",
    "                print(f\"Lỗi khi đọc row group {chunk_index}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Chờ tất cả các thread hoàn thành và tổng hợp kết quả\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                rows, count, processing_time = future.result()\n",
    "                total_rows += rows\n",
    "                total_count += count\n",
    "                total_processing_time += processing_time\n",
    "            except Exception as e:\n",
    "                print(f\"Lỗi khi xử lý chunk: {e}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Tổng số dòng: {total_rows}, tổng số từ 'french': {total_count}\")\n",
    "    print(f\"Tổng thời gian: {end_time - start_time:.2f} giây\")\n",
    "    print(f\"Tổng thời gian thực tế của các chunks: {total_processing_time:.2f} giây\")\n",
    "\n",
    "# Thực thi\n",
    "csv_file_path = r\"D:\\Data Engineer\\dwh and etl\\lesson\\lesson 2\\documents\\Talend\\Sync Hadoop to DWH\\citizens_data.csv\"\n",
    "parquet_file_path = r\"D:\\Data Engineer\\dwh and etl\\lesson\\lesson 2\\documents\\Talend\\Sync Hadoop to DWH\\citizens_data.parquet\"\n",
    "chunk_size = 50_00_000  # Kích thước chunk\n",
    "num_workers = 32         # Thử tăng lên 32 luồng nếu hệ thống hỗ trợ\n",
    "\n",
    "# Kiểm tra và tạo file Parquet nếu chưa có\n",
    "create_parquet_from_csv(csv_file_path, parquet_file_path)\n",
    "\n",
    "# Đọc và xử lý dữ liệu từ file Parquet\n",
    "read_and_process_parquet(parquet_file_path, chunk_size, num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320efc20-cb04-49c0-a67d-d7fc52660b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import time\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "def count_french_words(chunk):\n",
    "    \"\"\"Đếm từ 'french' trong chunk\"\"\"\n",
    "    french_word = \"french\"\n",
    "    return (\n",
    "        chunk['name'].str.lower().str.count(french_word).sum() +\n",
    "        chunk['country'].str.lower().str.count(french_word).sum()\n",
    "    )\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    \"\"\"Xử lý chunk và trả kết quả\"\"\"\n",
    "    try:\n",
    "        # Đếm từ 'french' trong chunk\n",
    "        count = count_french_words(chunk)\n",
    "        return len(chunk), count\n",
    "    except Exception as e:\n",
    "        return 0, 0\n",
    "\n",
    "def create_parquet_from_csv(csv_file, parquet_file):\n",
    "    \"\"\"Tạo file Parquet từ CSV nếu chưa có\"\"\"\n",
    "    if not os.path.exists(parquet_file):\n",
    "        df = pd.read_csv(csv_file, usecols=['name', 'country'])\n",
    "        table = pa.Table.from_pandas(df)\n",
    "        pq.write_table(table, parquet_file)\n",
    "\n",
    "def read_and_process_parquet(file_path, chunk_size, num_workers):\n",
    "    total_count = 0\n",
    "    total_rows = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Đọc dữ liệu từ file Parquet\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "\n",
    "    # Tạo pool đa luồng\n",
    "    with multiprocessing.Pool(processes=num_workers) as pool:\n",
    "        results = []\n",
    "\n",
    "        # Đọc dữ liệu theo từng row group (không tải toàn bộ vào RAM)\n",
    "        for chunk_index in range(parquet_file.num_row_groups):\n",
    "            try:\n",
    "                # Đọc 1 row group và chuyển đổi thành pandas DataFrame\n",
    "                chunk = parquet_file.read_row_group(chunk_index).to_pandas()[['name', 'country']]\n",
    "                \n",
    "                # Gửi chunk vào xử lý song song\n",
    "                results.append(pool.apply_async(process_chunk, (chunk,)))\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        # Chờ tất cả các chunk hoàn thành và tổng hợp kết quả\n",
    "        for result in results:\n",
    "            rows, count = result.get()\n",
    "            total_rows += rows\n",
    "            total_count += count\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Tổng thời gian xử lý: {end_time - start_time:.2f} giây\")\n",
    "    print(f\"Tổng số từ 'french': {total_count} trong {total_rows} dòng\")\n",
    "\n",
    "# Thực thi\n",
    "csv_file_path = r\"D:\\Data Engineer\\dwh and etl\\lesson\\lesson 2\\documents\\Talend\\Sync Hadoop to DWH\\citizens_data.csv\"\n",
    "parquet_file_path = r\"D:\\Data Engineer\\dwh and etl\\lesson\\lesson 2\\documents\\Talend\\Sync Hadoop to DWH\\citizens_data.parquet\"\n",
    "chunk_size = 50_00_000  # Kích thước chunk, bạn có thể điều chỉnh\n",
    "num_workers = multiprocessing.cpu_count()  # Tự động dùng số worker theo số lõi CPU\n",
    "\n",
    "# Kiểm tra và tạo file Parquet nếu chưa có\n",
    "create_parquet_from_csv(csv_file_path, parquet_file_path)\n",
    "\n",
    "# Đọc và xử lý dữ liệu từ file Parquet\n",
    "read_and_process_parquet(parquet_file_path, chunk_size, num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6945da17-3108-4b5d-9503-704c6fbdf157",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Parquet D:\\Data Engineer\\dwh and etl\\lesson\\lesson 2\\documents\\Talend\\Sync Hadoop to DWH\\citizens_data.parquet đã tồn tại, sẽ đọc từ đó.\n",
      "[Thread-0] Thời gian xử lý: 6.92s, CPU: 99.7% | RAM: 19.53274264685553% | Số từ 'french': 13517\n",
      "[Thread-10] Thời gian xử lý: 9.09s, CPU: 99.9% | RAM: 24.62432136293459% | Số từ 'french': 13487\n",
      "[Thread-20] Thời gian xử lý: 8.19s, CPU: 100.0% | RAM: 34.54536106426395% | Số từ 'french': 13577\n",
      "[Thread-30] Thời gian xử lý: 9.35s, CPU: 100.3% | RAM: 17.56504075096439% | Số từ 'french': 13471\n",
      "[Thread-40] Thời gian xử lý: 9.16s, CPU: 163.5% | RAM: 23.79931017054629% | Số từ 'french': 13487\n",
      "[Thread-50] Thời gian xử lý: 10.34s, CPU: 100.3% | RAM: 31.718791910825644% | Số từ 'french': 13684\n",
      "[Thread-60] Thời gian xử lý: 6.11s, CPU: 99.7% | RAM: 34.27832013771367% | Số từ 'french': 13456\n",
      "[Thread-70] Thời gian xử lý: 6.53s, CPU: 100.3% | RAM: 32.168476122985794% | Số từ 'french': 13557\n",
      "[Thread-80] Thời gian xử lý: 6.56s, CPU: 100.0% | RAM: 29.171535294947926% | Số từ 'french': 13312\n",
      "Tổng số dòng: 90000000, tổng số từ 'french': 1155146\n",
      "Tổng thời gian: 168.40 giây\n",
      "Tổng thời gian thực tế của các chunks: 659.80 giây\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pandas as pd\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "def count_french_words(chunk):\n",
    "    \"\"\"Đếm từ 'french' trong chunk\"\"\"\n",
    "    try:\n",
    "        french_word = \"french\"\n",
    "        # Calculate count in 'name' and 'country' columns using optimized string operations\n",
    "        return (\n",
    "            chunk['name'].str.lower().str.contains(french_word).sum() +\n",
    "            chunk['country'].str.lower().str.contains(french_word).sum()\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error in count_french_words: {e}\")\n",
    "        return 0\n",
    "\n",
    "def process_chunk(chunk, chunk_index):\n",
    "    \"\"\"Xử lý chunk và trả kết quả\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Đếm từ 'french' trong chunk\n",
    "        count = count_french_words(chunk)\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Log each chunk's processing time every 10th chunk\n",
    "        if chunk_index % 10 == 0:\n",
    "            cpu_percent, ram_percent = get_process_resources(os.getpid())\n",
    "            print(f\"[Thread-{chunk_index}] Thời gian xử lý: {processing_time:.2f}s, CPU: {cpu_percent}% | RAM: {ram_percent}% | Số từ 'french': {count}\")\n",
    "        \n",
    "        return len(chunk), count, processing_time\n",
    "    except Exception as e:\n",
    "        print(f\"[Thread-{chunk_index}] Lỗi: {e}\")\n",
    "        return 0, 0, 0\n",
    "\n",
    "def get_process_resources(pid=None):\n",
    "    \"\"\"Lấy tài nguyên của process (CPU, RAM)\"\"\"\n",
    "    try:\n",
    "        process = psutil.Process(pid)\n",
    "        cpu_percent = process.cpu_percent(interval=0.1)\n",
    "        memory_info = process.memory_info()\n",
    "        memory_percent = (memory_info.rss / psutil.virtual_memory().total) * 100\n",
    "        return cpu_percent, memory_percent\n",
    "    except psutil.NoSuchProcess:\n",
    "        return 0, 0\n",
    "\n",
    "def create_parquet_from_csv(csv_file, parquet_file):\n",
    "    \"\"\"Tạo file Parquet từ CSV nếu chưa có\"\"\"\n",
    "    if not os.path.exists(parquet_file):\n",
    "        print(f\"File {parquet_file} chưa tồn tại, đang chuyển đổi...\")\n",
    "        df = pd.read_csv(csv_file, usecols=['name', 'country'])\n",
    "        table = pa.Table.from_pandas(df)\n",
    "        pq.write_table(table, parquet_file)\n",
    "        print(f\"File Parquet {parquet_file} đã được tạo.\")\n",
    "    else:\n",
    "        print(f\"File Parquet {parquet_file} đã tồn tại, sẽ đọc từ đó.\")\n",
    "\n",
    "def read_and_process_parquet(file_path, chunk_size, num_workers):\n",
    "    total_count = 0\n",
    "    total_rows = 0\n",
    "    total_processing_time = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Đọc dữ liệu từ file Parquet\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = []\n",
    "        \n",
    "        # Read row groups with a chunk of rows for efficient memory usage\n",
    "        for chunk_index in range(parquet_file.num_row_groups):\n",
    "            try:\n",
    "                # Read a row group and convert it to pandas DataFrame (with specific columns)\n",
    "                chunk = parquet_file.read_row_group(chunk_index).to_pandas()[['name', 'country']]\n",
    "                \n",
    "                # Send chunk for processing\n",
    "                future = executor.submit(process_chunk, chunk, chunk_index)\n",
    "                futures.append(future)\n",
    "            except Exception as e:\n",
    "                print(f\"Lỗi khi đọc row group {chunk_index}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Wait for all threads to complete and aggregate results\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                rows, count, processing_time = future.result()\n",
    "                total_rows += rows\n",
    "                total_count += count\n",
    "                total_processing_time += processing_time\n",
    "            except Exception as e:\n",
    "                print(f\"Lỗi khi xử lý chunk: {e}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Tổng số dòng: {total_rows}, tổng số từ 'french': {total_count}\")\n",
    "    print(f\"Tổng thời gian: {end_time - start_time:.2f} giây\")\n",
    "    print(f\"Tổng thời gian thực tế của các chunks: {total_processing_time:.2f} giây\")\n",
    "\n",
    "# Thực thi\n",
    "csv_file_path = r\"D:\\Data Engineer\\dwh and etl\\lesson\\lesson 2\\documents\\Talend\\Sync Hadoop to DWH\\citizens_data.csv\"\n",
    "parquet_file_path = r\"D:\\Data Engineer\\dwh and etl\\lesson\\lesson 2\\documents\\Talend\\Sync Hadoop to DWH\\citizens_data.parquet\"\n",
    "chunk_size = 22_500_000  # Kích thước chunk\n",
    "num_workers = 4         # Sử dụng 32 worker threads nếu hệ thống hỗ trợ\n",
    "\n",
    "# Kiểm tra và tạo file Parquet nếu chưa có\n",
    "create_parquet_from_csv(csv_file_path, parquet_file_path)\n",
    "\n",
    "# Đọc và xử lý dữ liệu từ file Parquet\n",
    "read_and_process_parquet(parquet_file_path, chunk_size, num_workers)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
